---
title: "Spatial Analysis of Cardiovascular Diseases in the Northeastern United States"
author: "Michael Ontiveros"
output: 
    html_document:
        theme: cosmo 
        toc: true
        toc_float: true
        highlight: tango
        number_sections: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Details

+ Use the US Census Bureau's definition of the Northeast: [click here](https://www.wikiwand.com/en/Northeastern_United_States)
+ Fix the hole with missing data in our shapefile. Look at prior poster for relevant code. 
    + Shannon County (GEOID 46113) is now known as Oglala Lakota (GEOID 46102).
    + Wade Hamptom (GEOID 02270) is now known as Kusilvak (GEOID 02158).
+ Use 2017 data ACS CDC WONDER
+ check for correlation between covariates 
+ Next year, the US Census website—and not American FactFinder—will host the demographic data used for this case study.
+ Consider wrangling FIPS excel sheet found online
+ Reorder urbanization categories to correctly reflect "rural-ness."
+ Use head() or glimpse according to dim() output
+ Need to use average for lag effects (not weighted smoothing); this may be the same thing, verify! 

# Motivation

## Background

Cardiovascular diseases are currently the leading cause of death in the United States. Much is already known about the causes of most cardiovascular diseases. We intend to explore the environmental characteristics related to the mortality of these diseases. 

## Analysis goal

We will identify county-level characteristics that are associated with cardiovascular disease mortality in the Northeastern United States, giving particular attention to the potential effects of neighboring counties. 

## Learning objectives

The learning objectives include data wrangling of spatial data, spatial smoothing, and the creation and interpretation of spatially lagged models.

## Libraries

```{r}
library(reshape2)
library(tidyverse)
library(readxl)
library(sf)
library(tmap)
library(viridis)
library(spdep) #for neighborhood matrix, correlogram
library(car) #for variance inflation factor calculation
```

# What is the data?

We will be using data from two sources. 

We will be obtaining demographic and geographic information from the US Census Bureau.

Our analysis will use demographic information from the American Community Survey, which functions similarly to the US Census, but is conducted yearly. 

Two map files for this analysis will be considered. You can obtain these map files [here](https://www.census.gov/geographies/mapping-files/time-series/geo/carto-boundary-file.2017.html) and [here](https://www.census.gov/cgi-bin/geo/shapefiles/index.php). We will use the *2017 State, County, Minor Civil Division, and Incorporated Place FIPS Codes* provided [here](https://www.census.gov/geographies/reference-files/2017/demo/popest/2017-fips.html) to make identification of states and county more practical in our analysis. 

Our second source of data is underlying cause of death data from CDC WONDER. Simply put, each death record contains separate sections for the underlying cause of death and all multiple causes of death. The CDC compiles then compiles information from these death records to create the respective datasets.

The way we define deaths in our analysis is important to consider. We will be defining deaths with a cardiovascular disease listed as the underlying cause of death as a death as a result of a cardiovascular disease.

Not all deaths have a single, direct cause. For that very reason, all multiple causes of death are listed on death records. We've defined our cases in such a way to simplify our analysis and ensure that the cases we include in our study are the cases most likely to have died as a direct result from a cardiovascular disease. This is particularly important given the high prevalence of cardiovascular diseases.

The CDC's Wide-ranging ONline Data for Epidemiologic Research (WONDER) is a resource for a wide array of datasets surrounding the population health of the United States. There, you can find underlying cause of death data, which can be grouped by other characteristics such as sex, year, geographic region, and more. We will be examining diseases of the heart. These can be obtained using the following UCD-10 codes [here](https://wonder.cdc.gov/ucd-icd10.html):

+ I00-I09
+ I11
+ I13
+ I20-I51

# Data import

```{r, echo=FALSE}
# Temporary version of the OCS theme
theme_OCS <- theme(axis.text = element_text(size = 10, color="black",family = "Menlo", hjust = 0, face = "bold"),
        axis.line = element_line(size = 1, color = "black"),
        axis.title = element_text(size = 10, color="black",family = "Menlo", hjust = 0, face = "bold"),
        axis.ticks = element_line(size=1,color = "black"),
        panel.grid.major = element_line(color = "gray"),
        panel.grid.minor = element_line(color = "gray"),
        panel.border = element_blank(),
        panel.background = element_rect(fill=NA),
        plot.background = element_rect(fill = "#FFFFFF", color = "#FFFFFF"),
        plot.title = element_text(size = 14, color="black",family = "Menlo", hjust = 0, face = "bold"),
        plot.subtitle = element_text(color="black",family = "Menlo",size = 12, hjust = 0),
        legend.text = element_text(size=8,family = "Menlo", color="black"),
        legend.title = element_text(size=8, family = "Menlo", color="black", face = "bold", hjust = 0),
        legend.background = element_rect(fill="#FFFFFF", color = "#FFFFFF"),
        plot.caption = element_text(size=8, family = "Menlo", color="black", hjust = 0))
```

## United States county boundaries

Let's import the United States county boundaries data. Note that we are importing two different files. We must determine which of the two files to use. These two files have different resolutions.

The spatial data format we will be using for our analysis is known as the shapefile format. Shapefiles are a file format originated by Esri, the developers of the popular ArcGIS. Shapefiles can store a wide array of spatial data and are the most widely used format for spatial data. 

Using `reas_sf` by the `sf` package, we can read in the shapefiles for our analysis. 

```{r}
map_sf <- read_sf("cb_2017_us_county_20m/cb_2017_us_county_20m.shp")

# Need to show source for this document

map_sf_high_res <- read_sf("cb_2017_us_county_500k/cb_2017_us_county_500k.shp")

FIPS_codes <- read_excel("all-geocodes-v2017.xlsx")
```

Let's compare the boundaries of each file. 

If we plot the boundaries for the City of Baltimore (a county), we can see that there is substantially more detail in the larger of the two shapefiles. This could be useful when generating maps that are smaller in scale or when the resolution could impact an analysis.

```{r, echo=FALSE, warning=FALSE}
NY_shape <- map_sf %>% filter(STATEFP=="36" | STATEFP=="34" | STATEFP =="42")
NY_shape_high_res <- map_sf_high_res %>% filter(STATEFP=="36" | STATEFP=="34" | STATEFP =="42")

#There are 58 counties in California. Since three are shown, a warning regarding the other 55 is shown. 

NY_subset <- ggplot() +
  geom_sf(data=NY_shape, color="blue", fill="transparent", size = 0.05) +
  geom_sf(data=NY_shape_high_res, color="red", fill="transparent", size = 0.05) +
  geom_sf_text(data=NY_shape_high_res, aes(label = NAME), size = 2) +
  xlim(-74.45, -73.4) + 
  ylim(40.1, 41.1) +
  xlab("Longitude") +
  ylab("Latitude")+
  theme_void()
```

With that said, if we zoom out enough, the two shapefiles appear almost identical.

```{r, echo=FALSE, warning=FALSE}
NY_NJ_PA <- ggplot() +
  geom_sf(data=NY_shape, color="blue", fill="transparent", size = 0.05) +
  geom_sf(data=NY_shape_high_res, color="red", fill="transparent", size = 0.05) + 
  xlim(-80.5, -71.75) + 
  ylim(38.75, 45.25) +
  theme_void()

arrowA <- data.frame(x1 = 28, x2 = 35, y1 = 22, y2 = 47)
arrowB <- data.frame(x1 = 28, x2 = 35, y1 = 16, y2 = 3)

ggplot() +
    coord_equal(xlim = c(0, 70), ylim = c(0, 50), expand = FALSE) +
    annotation_custom(ggplotGrob(NY_NJ_PA), xmin=0, xmax = 35, ymin = 0, 
        ymax = 50) +
    annotation_custom(ggplotGrob(NY_subset), xmin=35, xmax = 70, ymin = 0, 
        ymax = 50) +
  geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2), data = arrowA, lineend = "round") +
  geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2), data = arrowB, lineend = "round") +
  labs(title = "Impact of resolution of spatial data on shape produced", subtitle = "Smaller shapefile in blue, larger shapefile in red") +
  theme_void()
```

We see that for certain applications the use of the larger shapefile is not only warranted, but necessary. Conversely, for other applications the use of the larger shapefile is not necessary.

Since we are not conducting an analysis that would have introduce bias or cause misclassification as a result of the shapefilefile we use, we are free to use the smaller of the two shapefiles. The smaller shapefile requires less computational resources and produces almost identical maps at the state level and beyond, making it ideal for getting through an analysis.

Often, analyzing data thoroughly can be severely hindered by the computational resources available. This is especially true with spatial applications of R. Unless an analysis requires that those computational resources be used, it is often more efficient and effective to proceed with an analysis in a way that is less computationally-intensive.

Again, because our analysis is not dependent on the absolute precision of the maps we use, but rather the relative location of boundaries, we are able to use the smaller, lower-resolution file. If, for example, our analysis involved the aggregation of points inside a county, we would not have this luxury; the lower resolution would likely lead to points near the true boundary being misclassified as being within the borders of a nearby county. 

## ACS data

Let's import the following datasets for 2017: 

TOTAL POPULATION
Universe: Total population  more information
2013-2017 American Community Survey 5-Year Estimates

[Click here to be directed to the address for download](https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?pid=ACS_17_5YR_B01003&prodType=table)

SELECTED CHARACTERISTICS OF HEALTH INSURANCE COVERAGE IN THE UNITED STATES
2013-2017 American Community Survey 5-Year Estimates

[Click here to be directed to the address for download](https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?pid=ACS_17_5YR_S2701&prodType=table)

MEDIAN INCOME IN THE PAST 12 MONTHS (IN 2017 INFLATION-ADJUSTED DOLLARS)
2013-2017 American Community Survey 5-Year Estimates

[Click here to be directed to the address for download](https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?pid=ACS_17_5YR_S1903&prodType=table)

MEDIAN AGE BY SEX
Universe: Total population
2013-2017 American Community Survey 5-Year Estimates

[Click here to be directed to the address for download](https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?pid=ACS_17_5YR_B01002&prodType=table)

RACE
Universe: Total population
2013-2017 American Community Survey 5-Year Estimates

[Click here to be directed to the address for download](https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?pid=ACS_17_5YR_B02001&prodType=table)

HISPANIC OR LATINO ORIGIN BY RACE
Universe: Total population
2013-2017 American Community Survey 5-Year Estimates

[Click here to be directed to the address for download](https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?pid=ACS_17_5YR_B03002&prodType=table)

```{r}
total_pop <- read_csv("ACS_17_5YR_B01003/ACS_17_5YR_B01003_with_ann.csv")

med_age <- read_csv("ACS_17_5YR_B01002/ACS_17_5YR_B01002_with_ann.csv")

race <- read_csv("ACS_17_5YR_B02001/ACS_17_5YR_B02001_with_ann.csv")

ethnicity <- read_csv("ACS_17_5YR_B03002/ACS_17_5YR_B03002_with_ann.csv")

med_inc <- read_csv("ACS_17_5YR_S1903/ACS_17_5YR_S1903_with_ann.csv")

perc_insured <- read_csv("ACS_17_5YR_S2701/ACS_17_5YR_S2701_with_ann.csv")
```

## Underlying cause of death data

Let's import the underlying cause of death data. 

```{r}
ucd_heart <- read_tsv("ucd_cardiovascular_diseases_county_2017.txt",
                      na = c("Suppressed","Unreliable", "Missing"))
glimpse(ucd_heart)
```

# Data wrangling

## ACS data

### Total population

```{r}
glimpse(total_pop)

colnames(total_pop) <- total_pop[1,]
total_pop <- total_pop[-1,]
total_pop$Id <- NULL
total_pop$`Margin of Error; Total` <- NULL
total_pop <- total_pop %>%
  rename("GEOID"=Id2, "Population"=`Estimate; Total`)
```

### Median age

```{r}
glimpse(med_age)

colnames(med_age)
colnames(med_age) <- med_age[1,]
med_age <- med_age[-1,]
med_age$Id <- NULL
med_age <- med_age %>%
  rename("GEOID"=Id2, "median_age"=`Estimate; Median age -- - Total:`) %>%
  select(GEOID, median_age)
class(med_age$median_age)
med_age$median_age <- as.numeric(med_age$median_age)
```

### Race 

```{r}
glimpse(race)

colnames(race)
colnames(race) <- race[1,]
race <- race[-1,]
race$Id <- NULL
```

```{r, error=TRUE}
race <- race %>%
  rename("GEOID"=Id2,
         "Total"=`Estimate; Total:`,
         "W_only"=`Estimate; Total: - White alone`,
         "BA_only"=`Estimate; Total: - Black or African American alone`,
         "A_only"=`Estimate; Total: - Asian alone`) %>%
  mutate(Perc_W_only=W_only/Total,
         Perc_BA_only=BA_only/Total,
         Perc_A_only=A_only/Total) %>%
  select(GEOID,
         Perc_W_only,
         Perc_BA_only,
         Perc_A_only)

class(race$`Estimate; Total:`)
class(race$`Estimate; Total: - White alone`)
class(race$`Estimate; Total: - Black or African American alone`)
class(race$`Estimate; Total: - Asian alone`)

race <- race %>%
  rename("GEOID"=Id2,
         "Total"=`Estimate; Total:`,
         "W_only"=`Estimate; Total: - White alone`,
         "BA_only"=`Estimate; Total: - Black or African American alone`,
         "A_only"=`Estimate; Total: - Asian alone`) %>%
  mutate("Total"=as.numeric(Total),
         "W_only"=as.numeric(W_only),
         "BA_only"=as.numeric(BA_only),
         "A_only"=as.numeric(A_only)) %>%
  mutate(Perc_W_only=W_only/Total,
         Perc_BA_only=BA_only/Total,
         Perc_A_only=A_only/Total) %>%
  select(GEOID,
         Perc_W_only,
         Perc_BA_only,
         Perc_A_only)
```

### Ethnicity

```{r, error=TRUE}
glimpse(ethnicity)

colnames(ethnicity)
colnames(ethnicity) <- ethnicity[1,]
ethnicity <- ethnicity[-1,]
ethnicity$Id <- NULL

ethnicity <- ethnicity %>%
  rename("GEOID"=Id2,
         "Total"=`Estimate; Total:`,
         "Hispanic_Latino"=`Estimate; Hispanic or Latino:`) %>%
  mutate(Perc_Hispanic_Latino=Hispanic_Latino/Total) %>%
  select(GEOID,
         Perc_Hispanic_Latino)

class(ethnicity$`Estimate; Total:`)
class(ethnicity$`Estimate; Hispanic or Latino:`)

ethnicity <- ethnicity %>%
  rename("GEOID"=Id2,
         "Total"=`Estimate; Total:`,
         "Hispanic_Latino"=`Estimate; Hispanic or Latino:`) %>%
  mutate("Total"=as.numeric(Total),
         "Hispanic_Latino"=as.numeric(Hispanic_Latino)) %>%
  mutate(Perc_Hispanic_Latino=Hispanic_Latino/Total) %>%
  select(GEOID,
         Perc_Hispanic_Latino)
```

### Median income

```{r}
glimpse(med_inc)

colnames(med_inc)
colnames(med_inc) <- med_inc[1,]
med_inc <- med_inc[-1,]
med_inc$Id <- NULL

med_inc <- med_inc %>%
  rename("GEOID"=Id2,
         "median_income"=`Median income (dollars); Estimate; Households`) %>%
  select(GEOID,
         median_income)

class(med_inc$median_income)
med_inc$median_income <- as.numeric(med_inc$median_income)

```

### Percent insured

```{r}
glimpse(perc_insured)

colnames(perc_insured)
colnames(perc_insured) <- perc_insured[1,]
perc_insured <- perc_insured[-1,]
perc_insured$Id <- NULL

perc_insured <- perc_insured %>%
  rename("GEOID"=Id2,
         "perc_insured"=`Percent Insured; Estimate; Civilian noninstitutionalized population`) %>%
  select(GEOID,
         perc_insured)

class(perc_insured$perc_insured)
perc_insured$perc_insured <- as.numeric(perc_insured$perc_insured)
```

## Underlying cause of death data

```{r}
ucd_heart$Notes <- NULL
ucd_heart$Population <- NULL
ucd_heart$`Crude Rate` <- NULL
ucd_heart$`2013 Urbanization` <- as.factor(ucd_heart$`2013 Urbanization`)
glimpse(ucd_heart)
```

*According to [this](https://www.cdc.gov/nchs/data/series/sr_02/sr02_166.pdf) document, the ordering should be something along these lines*

```{r}
ucd_heart <- ucd_heart %>%
  rename("GEOID"=`County Code`)
levels(ucd_heart$`2013 Urbanization`)
ucd_heart$`2013 Urbanization` <- factor(ucd_heart$`2013 Urbanization`,
       levels = c("NonCore (Nonmetro)",
                  "Micropolitan (Nonmetro)",
                  "Small Metro",
                  "Medium Metro",
                  "Large Fringe Metro",
                  "Large Central Metro"),
       ordered = TRUE)
levels(ucd_heart$`2013 Urbanization`)
```

## United States county boundaries

```{r}
map_sf <- st_transform(map_sf, 4326)

glimpse(map_sf)
```

```{r}
map_sf <- map_sf %>%
  left_join(ucd_heart, by="GEOID") %>%
  left_join(total_pop, by="GEOID")

class(map_sf$Deaths)
class(map_sf$Population)
class(map_sf$`2013 Urbanization`)

map_sf$Population <- as.numeric(map_sf$Population)

map_sf <- map_sf %>%
  mutate("Crude Rate"=round((Deaths/Population)*1e+06,3))
```

```{r}
glimpse(FIPS_codes)

head(FIPS_codes)

colnames(FIPS_codes)
FIPS_codes[4,1:6]
colnames(FIPS_codes) <- FIPS_codes[4,]
FIPS_codes <- FIPS_codes[-(1:4),]

glimpse(FIPS_codes)

sapply(FIPS_codes[,1:(length(colnames(FIPS_codes)))], class) # Apply this to other section of the code where checking of column class is necessary. 

FIPS_codes <- sapply(FIPS_codes[,1:length(colnames(FIPS_codes))], as.factor)

class(FIPS_codes)

FIPS_codes <- as.data.frame(FIPS_codes)

sapply(FIPS_codes[,1:(length(colnames(FIPS_codes))-1)], summary)

colnames(FIPS_codes)

FIPS_codes <- FIPS_codes %>%
  select(`Summary Level`,
         `State Code (FIPS)`,
         `County Code (FIPS)`,
         `Area Name (including legal/statistical area description)`) %>%
  rename("summary_level"=`Summary Level`,
         "state_code"=`State Code (FIPS)`,
         "county_code"=`County Code (FIPS)`,
         "area_name"=`Area Name (including legal/statistical area description)`) %>%
  sapply(as.character) %>%
  as.data.frame() %>% #Discuss how using sapply converted our dataframe into a matrix, what we are doing essentially is turning it back into a dataframe for manipulation
  mutate("GEOID"=paste0(state_code,county_code))

FIPS_codes_dfs <- FIPS_codes %>%
  group_split(summary_level)

FIPS_codes_dfs %>%
  lapply(`[`, 1, )

state_codes <- FIPS_codes_dfs[[2]]
glimpse(state_codes)
state_codes <- state_codes %>%
  select(state_code, area_name) %>%
  rename("state_name"=area_name)
glimpse(state_codes)
county_codes <- FIPS_codes_dfs[[3]]
glimpse(county_codes)
county_codes <- county_codes %>%
  select(-summary_level) %>%
  rename("county_name"=area_name)

# Mention how there is a heirarchy to these codes 

covariates <- county_codes %>%
  left_join(state_codes, by="state_code") %>%
  left_join(med_age, by="GEOID") %>%
  left_join(race, by="GEOID") %>%
  left_join(ethnicity, by="GEOID") %>%
  left_join(med_inc, by="GEOID") %>%
  left_join(perc_insured, by="GEOID")
  
```

*Now that we have the reference codes, we can update this section*

We can use information from the US Census Bureau to identify which codes correspond to the states of interest (click. [here](https://www.census.gov/geographies/reference-files/2017/demo/popest/2017-fips.html)).

```{r}
map_sf <- map_sf %>%
  left_join(covariates, by="GEOID")
```

```{r}
# Using "" around numbers to prevent 0 from disappearing from "09" later in the analysis. If the zero disappears, R does not recognize CT in maps or incorrectly classifies it later in the analysis. 

state_fp <- c("39", #OH
              "54", #WV
              "51", #VA
              "24", #MD
              "10", #DE
              "34", #NJ
              "42", #PA 
              "36", #NY
              "09", #CT
              "44", #RI
              "25", #MA
              "50", #VT
              "33", #NH
              "23" #ME
              )
```

```{r}
subset <- map_sf %>%
  filter(STATEFP %in% state_fp)
```

```{r}
contiguous_us <- map_sf %>% filter(!(STATEFP %in% c("02", #Alaska
                                                    "15", #Hawaii
                                                    "72")))
```

## Lag Covariates

We need to construct the neighborhood matrix in a way that does includes all the relationships between counties of interest, otherwise we will lose valuable information and could bias our results.

Let's create a neighborhood matrix that includes counties just outside of our region of interest. 

Five states border our region of interest. Let's include them in our neighborhood matrix.

```{r}
# "" around numbers for the same reason as mentioned before

surrounding_states <- c("37", #NC
                        "47", #TN
                        "21", #KY
                        "18", #IN
                        "26" #MI
                        )

matrix_counties <- c(state_fp, surrounding_states)

matrix_region <- map_sf %>%
  filter(STATEFP %in% matrix_counties)

matrix_region <- matrix_region %>%
  mutate(Region = ifelse(STATEFP %in% state_fp, "Region of Interest", "Surrounding States"))

```

Let's look at what we did visually. 

```{r}
ggplot(matrix_region) + 
  geom_sf(aes(fill=Region))
```

```{r}
# Would be neat to give an example of queen's vs rook's case matrices
matrix_region_sp <- as(matrix_region, 'Spatial')
nbrs_matrix_region_sp <- poly2nb(matrix_region_sp, queen = TRUE)
neighbors_sf <- as(nb2lines(nbrs_matrix_region_sp, coords = coordinates(matrix_region_sp)), 'sf')
neighbors_sf <- st_set_crs(neighbors_sf, st_crs(matrix_region))
```

```{r}
ggplot(matrix_region) + 
  geom_sf(aes(fill = Region), color=NA) +
  geom_sf(data = neighbors_sf, size=0.25, color="black") +
  ggtitle("Neighborhood Matrix", subtitle = "Queen's Case Adjacency")
```

#### Population weighted smoothing

With this matrix, we can create smoothed maps. Smoothing is the process of borrowing information from surrounding area-level units to create values with less variation.

Our regression will require that we create lagged variables, variables with values that are summaries of proximal regions. What we consider proximal is a matter of our analysis question.

Recall our adjacency matrix. We can borrow information from area-level units—in our case, counties—that are queen adjacent. Doing this would be considered smoothing.

If we smoothed nearest queen adjacent area-level units, we would be creating a first order lagged variable. 

Below we create a first order lagged variable.

```{r}
# First order lag

FFct_nb_w <- nb2listw(nbrs_matrix_region_sp,style="W", zero.policy = TRUE)
Wfull<-listw2mat(FFct_nb_w)

matrix_region$smoothed_pop <- Wfull %*% matrix_region$Population

ggplot(matrix_region) +
  geom_sf(aes(fill=smoothed_pop)) +
  scale_fill_gradientn(colours = viridis(10))
```

If we smoothed using the next nearest adjacent area-level units, we would be creating a second order lagged variable. 

The process for creating a second order lagged variable, and so on, is the same.

```{r}
# Second order lag 

nb_sec_lag_list <- nblag(nbrs_matrix_region_sp, maxlag = 2)
nb_sec_lag <- nb_sec_lag_list[[2]]

FFct_nb_w <- nb2listw(nb_sec_lag,style="W", zero.policy = TRUE)
Wfull<-listw2mat(FFct_nb_w)

matrix_region$smoothed_pop_sec_order <- Wfull %*% matrix_region$Population

ggplot(matrix_region) +
  geom_sf(aes(fill=smoothed_pop_sec_order)) +
  scale_fill_gradientn(colours = viridis(10))
```

We can also combine combine information from the nearest and next nearest adjacent area-level units. 

The code below uses the list of first and second order adjacent counties to do exactly that. 

```{r}
# Cumulative 1st and 2nd order

nb_cum_lag <- nblag_cumul(nb_sec_lag_list)

FFct_nb_w <- nb2listw(nb_cum_lag,style="W", zero.policy = TRUE)
Wfull<-listw2mat(FFct_nb_w)

matrix_region$smoothed_pop_cum_order <- Wfull %*% matrix_region$Population

ggplot(matrix_region) +
  geom_sf(aes(fill=smoothed_pop_cum_order)) +
  scale_fill_gradientn(colours = viridis(10))
```

If we take an extreme case, we can better visualize the implications such smoothing can have on our data. 

Below, we take an average of the first through tenth order adjacent counties. 

The end product is a map with much less variation. 

```{r}
# Extreme case

nb_ext_lag_list <- nblag(nbrs_matrix_region_sp, maxlag = 10)
nb_ext_cum_lag <- nblag_cumul(nb_ext_lag_list)

FFct_nb_w <- nb2listw(nb_ext_cum_lag,style="W", zero.policy = TRUE)
Wfull<-listw2mat(FFct_nb_w)

matrix_region$smoothed_pop_ext_cum_order <- Wfull %*% matrix_region$Population

ggplot(matrix_region) +
  geom_sf(aes(fill=smoothed_pop_ext_cum_order)) +
  scale_fill_gradientn(colours = viridis(10))
```

If we plot the difference of the smoothed population from the actual population, we see that there is systematic overestimation and underestimation throughout this map. This is because of the effect smoothing has on our values. The more counties we use to generate an average summary value for a county, the more the summary value is alike to the average of the entire region. The reduced variation has both a cost and a benefit. 

```{r}
matrix_region <- matrix_region %>%
  mutate(Bias = Population - smoothed_pop_ext_cum_order) %>%
  mutate(Bias_cat = cut(Bias, breaks = 7, include.lowest = TRUE, ordered_result = TRUE)) %>%
  mutate("Bias Class" = ifelse(sign(Bias)==-1, "Overestimation", ifelse(sign(Bias)==1, "Underestimation", "Neither"))) 

summary(matrix_region$Bias)

ggplot(matrix_region) +
  geom_sf(aes(fill=Bias_cat)) +
  scale_fill_manual(values=c(viridis(7)))

ggplot(matrix_region) +
  geom_sf(aes(fill=`Bias Class`)) +
  scale_fill_manual(values = c(viridis(2)))
```

We can think of this in a different way. What if we were instead analyzing data for the entire contiguous US? If we smoothed enough so that the entire northeast region had a very similar population per county, the county values would be very similar to the average of the region. This idea still holds when we are analyzing a smaller region: if we smooth values enough, the values we produce tend to reflect the average of the region we our analyzing.

```{r}
# This would be a great example to show an error if you don't remove the one value without population. The matrix algebra would not work! 

contiguous_us <- contiguous_us %>% filter(is.na(Population)==FALSE)
contiguous_us_sp <- as(contiguous_us, 'Spatial')
nbrs_contiguous_us_sp <- poly2nb(contiguous_us_sp, queen = TRUE)

nb_ext_lag_list_contiguous <- nblag(nbrs_contiguous_us_sp, maxlag = 10)
nb_ext_cum_lag_contiguous <- nblag_cumul(nb_ext_lag_list_contiguous)

A <- nb2listw(nb_ext_cum_lag_contiguous,style="W", zero.policy = TRUE)
Wfulle <-listw2mat(A)

contiguous_us$smoothed_pop_ext_cum_order <- Wfulle %*% contiguous_us$Population

ggplot(contiguous_us) +
  geom_sf(aes(fill=smoothed_pop_ext_cum_order), color="transparent") + 
  scale_fill_gradientn(colors = viridis(10))
```

Broadly speaking, smoothing makes values look more like the global average, thereby reducing the global variance, exchanging exactitude in a map for the easier visualization. Smoothing creates fictitious values that are more than often overestimates or understimates of the true value. Despite this, we can use smoothing to advance our analysis with proper caution. Smoothing can help produce more interpretable visualizations as well as lagged variables that can be used in both inferential and predictive models. 

Using the code we used earlier, we can create a function that produces a lagged covariate. This is more efficient that manually creating each variable with the same code and prevents errors. Conversely, errors in our function can be systematically applied to each of our covariates. Since we already have the code and are assured that it functions as we intend, this is not a problem.

```{r}
lag_creator <- function(vector){
  lag_vector <- Wfull %*% vector
  lag_vector
}

sum(complete.cases(covariates)==FALSE)

covariates[which(complete.cases(covariates)==FALSE),]

matrix_region <- matrix_region %>%
  mutate("median_age_lag"=lag_creator(median_age),
         "Perc_W_only_lag"=lag_creator(Perc_W_only),
         "Perc_BA_only_lag"=lag_creator(Perc_BA_only),
         "Perc_Hispanic_Latino_lag"=lag_creator(Perc_Hispanic_Latino),
         "median_income_lag"=lag_creator(median_income),
         "perc_insured_lag"=lag_creator(perc_insured),
         )

```
# Exploratory data analysis 

We can explore our data in different ways now that we've merged our data to one of the maps we've imported. 

The great thing about using sf objects is that they work very well with the `tidyverse` and `ggplot` packages. The code required to visualize and summarize our spatial data is extremely similar, if not identical to that used in non-spatial applications. 

Let's create a map of the crude mortality rate of cardiovascular diseases across the contiguous United States to help identify trends beyond our region of interest. 

It appears that rural regions of the United States may have greater crude mortality rates. 

```{r}
ggplot(contiguous_us) +
  geom_sf(aes(fill=log(`Crude Rate`)), color="black", lwd=0.1) +
  scale_fill_gradientn(colours = viridis(5)) +
  labs(title = "Crude Rate", subtitle = "Contiguous United States", caption = "Calculated with UCD data from CDC WONDER") +
  theme_OCS
```

If we look at the Northeastern United States... *update*

```{r}
ggplot(subset) +
  geom_sf(aes(fill=`Crude Rate`), color="black", lwd=0.1) +
  scale_fill_gradientn(colours = viridis(10)) +
  ggtitle("Crude Rate", subtitle = "Northeastern United States") + 
  theme_OCS
```

# Data analysis 

```{r}
model1 <- glm(Deaths ~ median_age + 
              Perc_W_only +
              Perc_BA_only + 
              Perc_Hispanic_Latino + 
              median_income +
              perc_insured, 
            family="poisson",
            offset = log(Population),
            data=matrix_region,
            na.action = na.exclude)

model2 <- glm(Deaths ~ median_age + 
              median_age_lag + 
              Perc_W_only + 
              Perc_W_only_lag +
              Perc_BA_only + 
              Perc_BA_only_lag + 
              Perc_Hispanic_Latino + 
              Perc_Hispanic_Latino_lag + 
              median_income +
              median_income_lag +
              perc_insured +
              perc_insured_lag, 
            family="poisson",
            offset = log(Population),
            data=matrix_region,
            na.action = na.exclude)
```

The presence of multicollinearity means that there is correlation between variables. However, high correlation does not mean that there is multicollinearity. 

Let's produce some correlation matrices and compare them with computed variance inflation factors for our variables. 

```{r}
melt_cor1 <- as.data.frame(matrix_region) %>%
  select(median_age,
         Perc_W_only,
         Perc_BA_only,
         Perc_Hispanic_Latino,
         median_income,
         perc_insured) %>%
  cor() %>%
  round(3) %>%
  reshape2::melt()

colnames(melt_cor1)

melt_cor1 %>%
  ggplot(aes(x=Var1, y=Var2, fill=value, label=value)) +
  geom_tile() +
  scale_fill_gradient2(low="red", mid="white", high="green") + 
  geom_text() + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
        axis.text.y = element_text(angle = 45, vjust = 1, hjust = 1)) +
  ggtitle("Correlation matrix, direct variables") + 
  xlab("") +
  ylab("")
```

```{r}
melt_cor2 <- as.data.frame(matrix_region) %>%
  select(median_age_lag,
         Perc_W_only_lag,
         Perc_BA_only_lag,
         Perc_Hispanic_Latino_lag,
         median_income_lag,
         perc_insured_lag) %>%
  cor() %>%
  round(3) %>%
  reshape2::melt()

colnames(melt_cor2)

melt_cor2 %>%
  ggplot(aes(x=Var1, y=Var2, fill=value, label=value)) +
  geom_tile() +
  scale_fill_gradient2(low="red", mid="white", high="green") + 
  geom_text() + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
        axis.text.y = element_text(angle = 45, vjust = 1, hjust = 1)) +
  ggtitle("Correlation matrix, direct/lagged variables") + 
  xlab("") +
  ylab("")
```

```{r}
vif(model1)
```

```{r}
vif(model2)
```

```{r}
sum(is.na(matrix_region$Deaths))
length(model1$residuals)
length(model2$residuals)
```

```{r}
#Are the residuals being plotted correctly? 

matrix_region <- matrix_region %>%
  mutate("residuals1"=residuals.glm(model1, "pearson"))

matrix_region %>%
  ggplot() +
  geom_sf(aes(fill=residuals1), color="transparent") + 
  scale_fill_gradient2(low="white", high="red")

matrix_region <- matrix_region %>%
  mutate("residuals2"=residuals.glm(model2, "pearson"))

matrix_region %>% 
  ggplot() +
  geom_sf(aes(fill=residuals2), color="transparent") + 
  scale_fill_gradient2(low="white", high="red")

matrix_region <- matrix_region[is.na(matrix_region$residuals1)==FALSE,]
nbrs_matrix_region_sp <- poly2nb(matrix_region, queen = TRUE)
FFct_nb_w_test <- nb2listw(nbrs_matrix_region_sp,style="W", zero.policy = TRUE)

cor8m2 <-sp.correlogram(neighbours=FFct_nb_w_test$neighbours,var=matrix_region$residuals1,order=15,method="I",style="W", zero.policy = TRUE)
cor8m3 <-sp.correlogram(neighbours=FFct_nb_w_test$neighbours,var=matrix_region$residuals2,order=15,method="I",style="W", zero.policy = TRUE)
plot(cor8m2,ylim=c(-0.1, 0.4),main="Residuals from model 1")
plot(cor8m3,ylim=c(-0.1, 0.4),main="Residuals from model 2")

```

# Summary of results

# Code garage

```{r}
tm_shape(subset) +
	tm_polygons(c("Deaths", "Population", "Crude Rate", "2013 Urbanization"),
				style=c("pretty","pretty","pretty", "pretty"), breaks=list(NULL, NULL, NULL, NULL),
				palette=list("Oranges", "Purples", "Reds", "Blues"),
				border.col = "white",
				title=c("Deaths", "Population", "Crude Rate", "2013 Urbanization")) + tm_facets(as.layers = TRUE)
```
				
# Downloaded data

Total population 5 year 2017
https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?pid=ACS_17_5YR_B01003&prodType=table

https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?pid=ACS_17_5YR_S2701&prodType=table
SELECTED CHARACTERISTICS OF HEALTH INSURANCE COVERAGE IN THE UNITED STATES  more information
2013-2017 American Community Survey 5-Year Estimates

https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?pid=ACS_17_5YR_S1903&prodType=table
MEDIAN INCOME IN THE PAST 12 MONTHS (IN 2017 INFLATION-ADJUSTED DOLLARS)  more information
2013-2017 American Community Survey 5-Year Estimates

https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?pid=ACS_17_1YR_B01002&prodType=table
MEDIAN AGE BY SEX
Universe: Total population  more information
2017 American Community Survey 1-Year Estimates

https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?pid=ACS_17_5YR_B02001&prodType=table
RACE
Universe: Total population  more information
2013-2017 American Community Survey 5-Year Estimates

https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?pid=ACS_17_5YR_B03002&prodType=table
HISPANIC OR LATINO ORIGIN BY RACE
Universe: Total population  more information
2013-2017 American Community Survey 5-Year Estimates

UCD Death Data—CDC Wonder
https://wonder.cdc.gov/ucd-icd10.html

