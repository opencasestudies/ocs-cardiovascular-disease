---
title: "Spatial Analysis of Cardiovascular Diseases in the Northeastern United States"
output: 
    html_document:
        theme: cosmo 
        toc: true
        toc_float: true
        highlight: tango
        number_sections: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Details

+ Use the US Census Bureau's definition of the Northeast: [click here](https://www.wikiwand.com/en/Northeastern_United_States)
+ Next year, the US Census website—and not American FactFinder—will host the demographic data used for this case study.
+ Consider wrangling FIPS excel sheet found online

pivot wide pivot longr in tidyr for reshape melt, change font theme

reinforce weight matrix multiple to average--Leah Meeting 

explanation of edge vs centroid.

convert dist covariate to km 

create function to wrangle data

explain why geometries must be constant. 

Need to explain EPSG codes. check out [this website](http://support.virtual-surveyor.com/en/support/solutions/articles/1000261353-what-is-an-epsg-code-)

# Motivation

## Background

Cardiovascular diseases are currently the leading cause of death in the United States. Much is already known about the causes of most cardiovascular diseases. We intend to explore potential environmental characteristics related to the mortality of cardiovascular diseases. 

## Analysis goal

We will identify county-level characteristics that are associated with cardiovascular disease mortality in the Northeastern United States, giving particular attention to the potential effects of neighboring counties. 

## Learning objectives

**MICHAEL** This needs to be revisited to include something about the assumptions made about non-spatial models and how we account for when these assumptions fail in the following case study.

The learning objectives include data wrangling of spatial data, spatial smoothing, and the creation and interpretation of spatially lagged models.

## Libraries

```{r}
library(MASS)
library(reshape2)
library(tidyverse)
library(readxl)
library(sf)
library(tmap)
library(viridis)
library(spdep) #for neighborhood matrix, correlogram
library(car) #for variance inflation factor calculation
```

# What is the data?

We will be using data from two sources. 

We will be obtaining demographic and geographic information from the US Census Bureau.

Our analysis will use demographic information from the American Community Survey, which functions similarly to the US Census, but is conducted yearly. 

Two map files for this analysis will be considered. You can obtain these map files [here](https://www.census.gov/geographies/mapping-files/time-series/geo/carto-boundary-file.2017.html) and [here](https://www.census.gov/cgi-bin/geo/shapefiles/index.php). We will use the *2017 State, County, Minor Civil Division, and Incorporated Place FIPS Codes* provided [here](https://www.census.gov/geographies/reference-files/2017/demo/popest/2017-fips.html) to make identification of states and counties more practical in our analysis. 

Our second source of data is underlying cause of death data from CDC WONDER. Simply put, each death record contains separate sections for the underlying cause of death and all multiple causes of death. The CDC compiles information from these death records to create the respective datasets.

The way we define deaths in our analysis is important to consider. We will be defining deaths with a cardiovascular disease listed as the underlying cause of death as a death as a result of a cardiovascular disease.

Not all deaths have a single, direct cause. For that very reason, multiple causes of death are listed on death records. We've defined our cases in such a way to simplify our analysis and ensure that the cases we include in our study are the cases most likely to have died as a direct result from a cardiovascular disease. This is particularly important given the high prevalence of cardiovascular diseases.

The CDC's Wide-ranging ONline Data for Epidemiologic Research (WONDER) is a resource for a wide array of datasets surrounding the population health of the United States. There, you can find underlying cause of death data, which can be grouped by other characteristics such as sex, year, geographic region, and more. We will be examining diseases of the heart. These can be obtained using the following UCD-10 codes [here](https://wonder.cdc.gov/ucd-icd10.html):

+ I00-I09
+ I11
+ I13
+ I20-I51

# Data import

```{r, echo=FALSE}
# Temporary version of the OCS theme
theme_OCS <- theme(axis.text = element_text(size = 10, color="black",family = "Menlo", hjust = 0, face = "bold"),
        axis.line = element_line(size = 1, color = "black"),
        axis.title = element_text(size = 10, color="black",family = "Menlo", hjust = 0, face = "bold"),
        axis.ticks = element_line(size=1,color = "black"),
        panel.grid.major = element_line(color = "gray"),
        panel.grid.minor = element_line(color = "gray"),
        panel.border = element_blank(),
        panel.background = element_rect(fill=NA),
        plot.background = element_rect(fill = "#FFFFFF", color = "#FFFFFF"),
        plot.title = element_text(size = 14, color="black",family = "Menlo", hjust = 0, face = "bold"),
        plot.subtitle = element_text(color="black",family = "Menlo",size = 12, hjust = 0),
        legend.text = element_text(size=8,family = "Menlo", color="black"),
        legend.title = element_text(size=8, family = "Menlo", color="black", face = "bold", hjust = 0),
        legend.background = element_rect(fill="#FFFFFF", color = "#FFFFFF"),
        plot.caption = element_text(size=8, family = "Menlo", color="black", hjust = 0))
```

Let's import the data for this analysis. 

## Boundaries and Reference Codes

We'll begin by importing the United States county boundaries data. Note that we are importing two different files. We must determine which of the two files to use. These two files have different resolutions.

The spatial data format we will be using for our analysis is known as the shapefile format. Shapefiles are a file format originated by Esri, the developers of the popular ArcGIS. Shapefiles can store a wide array of spatial data and are the most widely used format for spatial data. 

Using `read_sf` by the `sf` package, we can read in the shapefiles for our analysis. 

The shapefiles for this analysis can be found [here](https://www.census.gov/geographies/mapping-files/2017/geo/kml-cartographic-boundary-files.html).

```{r}
map_sf <- read_sf("cb_2017_us_county_20m/cb_2017_us_county_20m.shp")
st_crs(map_sf)
map_sf <- st_transform(map_sf, 2163)

map_sf_high_res <- read_sf("cb_2017_us_county_500k/cb_2017_us_county_500k.shp")
st_crs(map_sf)
map_sf_high_res <- st_transform(map_sf_high_res, 2163)
```

In addition to the shapefiles, we will import reference codes so that we can identify areas efficiently. We'll discuss in more detail later why having reference codes in datasets is helpful when dealing with spatial data. 

```{r}
FIPS_codes <- read_excel("all-geocodes-v2017.xlsx")
```

Let's compare the boundaries of each file. 

If we plot each, we can see that there is substantially more detail in the larger of the two shapefiles. This could be useful when generating maps that are smaller in scale or when the resolution could impact an analysis.

```{r, echo=FALSE, warning=FALSE}
NY_shape <- map_sf %>% filter(STATEFP=="36" | STATEFP=="34" | STATEFP =="42")
NY_shape_high_res <- map_sf_high_res %>% filter(STATEFP=="36" | STATEFP=="34" | STATEFP =="42")

#There are 58 counties in California. Since three are shown, a warning regarding the other 55 is shown. 

NY_subset <- ggplot() +
  geom_sf(data=NY_shape, color="blue", fill="transparent", size = 0.5) +
  geom_sf(data=NY_shape_high_res, color="red", fill="transparent", size = 0.5) +
  geom_sf_text(data=NY_shape_high_res, aes(label = NAME), size = 2.5) +
  xlim(2125000, 2175000) + 
  ylim(-150000, -100000) +
  xlab("Longitude") +
  ylab("Latitude")+
  theme_void()
```

With that said, if we zoom out enough the two shapefiles are almost indiscernable from one another.

```{r, echo=FALSE, warning=FALSE}
NY_NJ_PA <- ggplot() + 
  geom_sf(data=NY_shape, color="blue", fill="transparent", size = 0.1) +
  geom_sf(data=NY_shape_high_res, color="red", fill="transparent", size = 0.1) +
  xlim(1592058, 2303517) + 
  ylim(-391238.6, 344335) +
  theme_void()

arrowA <- data.frame(x1 = 28, x2 = 35, y1 = 23, y2 = 47)
arrowB <- data.frame(x1 = 28, x2 = 35, y1 = 18, y2 = 3)

ggplot() +
    coord_equal(xlim = c(0, 70), ylim = c(0, 50), expand = FALSE) +
    annotation_custom(ggplotGrob(NY_NJ_PA), xmin=0, xmax = 35, ymin = 0, 
        ymax = 50) +
    annotation_custom(ggplotGrob(NY_subset), xmin=35, xmax = 70, ymin = 0, 
        ymax = 50) +
  geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2), data = arrowA, lineend = "round") +
  geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2), data = arrowB, lineend = "round") +
  labs(title = "Impact of resolution of spatial data on shape produced", subtitle = "Smaller shapefile in blue, larger shapefile in red") +
  theme_void()
```

We see that for certain applications the use of the larger shapefile is not only warranted, but necessary. Conversely, for other applications the use of the larger shapefile is not necessary.

Since we are not conducting an analysis that could introduce bias or cause misclassification as a result of the shapefilefile we use, we are free to use the smaller of the two shapefiles. The smaller shapefile requires less computational resources and produces almost identical maps at the state level and beyond, making it preferable for this analysis.

Often, analyzing data thoroughly can be severely hindered by the computational resources available. This is especially true with spatial applications of R. Unless an analysis requires that those computational resources be used, it is often more efficient and effective to proceed with an analysis in a way that is less computationally-intensive.

Again, because our analysis is not dependent on the absolute precision of the maps we use, but rather the relative location of boundaries, we are able to use the smaller, lower-resolution file. If, for example, our analysis involved the aggregation of points inside a county, we would not have this luxury; the lower resolution would likely lead to points near the true boundary being misclassified as being within the borders of a nearby county. 

## ACS data

Let's import the following datasets for 2017: 

---

TOTAL POPULATION

B01003

2013-2017 American Community Survey 5-Year Estimates

[Click here to be directed to the address for download](https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?pid=ACS_17_5YR_B01003&prodType=table)

---

SELECTED CHARACTERISTICS OF HEALTH INSURANCE COVERAGE IN THE UNITED STATES

S2701

2013-2017 American Community Survey 5-Year Estimates

[Click here to be directed to the address for download](https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?pid=ACS_17_5YR_S2701&prodType=table)

---


MEDIAN INCOME IN THE PAST 12 MONTHS (IN 2017 INFLATION-ADJUSTED DOLLARS)

S1903

2013-2017 American Community Survey 5-Year Estimates

[Click here to be directed to the address for download](https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?pid=ACS_17_5YR_S1903&prodType=table)

---

MEDIAN AGE BY SEX

B01002

2013-2017 American Community Survey 5-Year Estimates

[Click here to be directed to the address for download](https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?pid=ACS_17_5YR_B01002&prodType=table)

---

RACE

B02001

2013-2017 American Community Survey 5-Year Estimates

[Click here to be directed to the address for download](https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?pid=ACS_17_5YR_B02001&prodType=table)

---

HISPANIC OR LATINO ORIGIN BY RACE

B03002

2013-2017 American Community Survey 5-Year Estimates

[Click here to be directed to the address for download](https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?pid=ACS_17_5YR_B03002&prodType=table)

---

These datasets will serve as predictors in the model we create. 

**MICHAEL** These need to be ordered properly. 

```{r}
total_pop <- read_csv("ACS_17_5YR_B01003/ACS_17_5YR_B01003_with_ann.csv")

med_age <- read_csv("ACS_17_5YR_B01002/ACS_17_5YR_B01002_with_ann.csv")

race <- read_csv("ACS_17_5YR_B02001/ACS_17_5YR_B02001_with_ann.csv")

ethnicity <- read_csv("ACS_17_5YR_B03002/ACS_17_5YR_B03002_with_ann.csv")

med_inc <- read_csv("ACS_17_5YR_S1903/ACS_17_5YR_S1903_with_ann.csv")

perc_insured <- read_csv("ACS_17_5YR_S2701/ACS_17_5YR_S2701_with_ann.csv")
```

## Underlying cause of death data

Let's import the underlying cause of death data. 

This dataset will provide data for the counts in our Poisson regression model .

```{r}
ucd_heart <- read_tsv("ucd_cardiovascular_diseases_county_2017.txt",
                      na = c("Suppressed","Unreliable", "Missing"))
glimpse(ucd_heart)
```

Now that we've imported the data, we can wrangle the data for our analysis needs. 

# Data wrangling

We cannot wrangle a dataset for a specific purpose without first developing a clear idea of its components. This is particularly true when there are many columns. 

The `head()` and `glimpse()` functions from the `utils` and `tidyverse` packages are particularly useful for that purpose. Each function has an advantage and disadvantage over the other. The `head()` function returns the first several rows of data; this output, while very easy to read for datasets with few dimensions, is too messy to interpret in the presence of more than a handful of columns of data. The `glimpse()` function returns the first several rows for each and every column of a dataframe; this output, while thorough, can generate a lot of output that is difficult to sift through.

Rather than use one exclusively, we will use whichever is more beneficial for the dataset we are working with.

Using the `dim()` function from `base`, we can make that determination. 

If a function has few dimensions, we will use the `head()` function. If a function has more than 5 columns, we will use the `glimpse()` function. If a function too many dimensions to sift through reasonably, we will use regular expressions to identify our variable of interest. 
## ACS data

We'll begin by wrangling the American Community Survey data. 

### Total population

Let's prepare the dataset with the total population for each county. 

```{r}
ACS_wranglr <- function(dataframe, old_name, new_name){
  colnames(dataframe) <- dataframe[1,]
  dataframe <- dataframe[-1,]
  colnames(dataframe)[colnames(dataframe)=="Id2"] <- "FIPS"
  colnames(dataframe)[colnames(dataframe)==old_name] <- new_name
  dataframe <- dataframe %>%
    select(FIPS, new_name)
  dataframe
}

ACS_wranglr(total_pop, "Estimate; Total", "Population")
```

```{r}
dim(total_pop)
```

This dataset has `r dim(total_pop)[2]` columns and `r dim(total_pop)[1]` rows. Given the small number of columns, it's more expedient to use the `head()` function to first visualize the dataframe. 

```{r}
head(total_pop)
```

A few things can be said about the dataframe from the above output. 

The column names of this dataset correspond to a codebook and are difficult to interpret on their own. The first row of the dataframe uses more interpretable column names. In addition to this, not all the columns have information we need for our analysis.

We need to obtain practical column names and keep the information we need, mainly the county and corresponding estimates. 

Below we do just that. We used the first row to replace the column names. Then, we rid ourselves of unnecessary columns. Alternatively, we could have used the `select()` function by `dplyr`, a part of the `tidyverse`, to rid ourselves of the unnecessary columns.  

```{r}
colnames(total_pop) <- total_pop[1,]
total_pop <- total_pop[-1,]
```

If we look at our dataframe now, each population estimate is coupled to a five digit code. Each US county has a unique identifier, called a FIPS code, that allows tabular data to be joined to spatial data. 

```{r}
head(total_pop)
```

Eventually, we will be joining this dataframe to our spatial data. Rather than join our tabular data based upon the name of each county, we will use this five digit code to join our tabular data to our spatial data. This avoids having to use regular expressions to make the names of counties parsimonious before joining, making such joins both more efficient and **accurate**. 

*MICHAEL:LEAH* In the blurb above, I'm not sure if **accurate** is appropriate. Is there a word commonly used in data science that describes correctly attributing information to an observation/row. In much of the spatial work I've done, such joins are incredibly important. I think it'd be extremely beneficial here if we could highlight the importance of reference codes using data science language.

For wrangling and regression analysis purposes, we need to rename our columns. Our other dataframes, also from the American Community Survey, will also use unique FIPS codes to refer to counties. 

```{r}
total_pop <- total_pop %>%
  rename("FIPS"=Id2, "Population"=`Estimate; Total`)

total_pop$Population <- as.numeric(total_pop$Population)
```

### Median age

Let's wrangle our dataset representing the median age of each county. 

```{r}
dim(med_age)
```

This dataset, because of the number of columns it has (`r dim(med_age)[2]`), would be better examined with `glimpse()`. 

```{r}
glimpse(med_age)
```

Like the prior dataset, this dataset uses column names that correspond to a codebook. If we look closely, it appears that the first observation of each column, that is the first row, contains more practical names. Our wrangling approach is therefore pretty similar.

```{r}
colnames(med_age)
colnames(med_age) <- med_age[1,]
med_age <- med_age[-1,]
med_age <- med_age %>%
  rename("FIPS"=Id2, "median_age"=`Estimate; Median age -- - Total:`) %>%
  dplyr::select(FIPS, median_age)
class(med_age$median_age)
med_age$median_age <- as.numeric(med_age$median_age)
```

### Race 

By now, its apparent that each of the remaining datasets will require similar steps to wrangle. We should begin to think about creating a function that will help streamline the wrangling process. Such a function would take some time to create to ensure that it produces the output we intend.

If we had to wrangle many similarly sourced datasets, this could make the wrangling process much more efficient and could save hours of work that could be spent directly analyzing the data. If done properly, such a function could actually prevent human error that commonly emerges with tasks that require repitition.

The problem with such a function is that knowledge of the steps involved in the wrangling process is required in advance. Without such knowledge, mistakes in our function could be compounded across our datasets. Since we only have a handful of datasets to wrangle, our time would be better spent manually wrangling each dataset. 
The following dataset provides a great example of how a function is only as good as the foresight involved in its creation. 

The dataset has `r dim(race)[2]` columns.

```{r}
dim(race)
```

Using `glimpse()`, it becomes apparent that this dataset also uses impractical column names which could be made more practical with replacement by names included in the first row. 

```{r}
glimpse(race)
```

We replace the impractical column names with more practical column names.

```{r}
colnames(race)
colnames(race) <- race[1,]
race <- race[-1,]
```

We need to create several predictors with information provided by this dataset. To do so, we'll use several `dplyr` verbs.

Before getting into detail, note that the code below generates an error. 

```{r, error=TRUE}
race <- race %>%
  rename("FIPS"=Id2,
         "Total"=`Estimate; Total:`,
         "W_only"=`Estimate; Total: - White alone`,
         "BA_only"=`Estimate; Total: - Black or African American alone`,
         "A_only"=`Estimate; Total: - Asian alone`) %>%
  mutate(Perc_W_only=W_only/Total,
         Perc_BA_only=BA_only/Total,
         Perc_A_only=A_only/Total) %>%
  dplyr::select(FIPS,
         Perc_W_only,
         Perc_BA_only,
         Perc_A_only)
```

If we check the class of each column, we find that each of the columns we treat numerically in the above code are of character class. 

```{r}
class(race$`Estimate; Total:`)
class(race$`Estimate; Total: - White alone`)
class(race$`Estimate; Total: - Black or African American alone`)
class(race$`Estimate; Total: - Asian alone`)
```

Using the `mutate()` function, we can coerce these columns to `numeric` class. This lets us complete the calculations we originally intended. 

Below we do just that. First, we rename the columns for practicality. Second, we coerce the columns to `numeric` class. Then, we calculate the predictors we'll be using for our regression analysis. Finally, we eliminate information that would not be relevant to our analysis with `select()`.

```{r}
race <- race %>%
  rename("FIPS"=Id2,
         "Total"=`Estimate; Total:`,
         "W_only"=`Estimate; Total: - White alone`,
         "BA_only"=`Estimate; Total: - Black or African American alone`,
         "A_only"=`Estimate; Total: - Asian alone`) %>%
  mutate("Total"=as.numeric(Total),
         "W_only"=as.numeric(W_only),
         "BA_only"=as.numeric(BA_only),
         "A_only"=as.numeric(A_only)) %>%
  mutate(Perc_W_only=W_only/Total,
         Perc_BA_only=BA_only/Total,
         Perc_A_only=A_only/Total) %>%
  dplyr::select(FIPS,
         Perc_W_only,
         Perc_BA_only,
         Perc_A_only)
```

While a function to wrangle our datasets could have been created to identify and deal with such issues, it could have only been done so with advanced knowledge of the problems that may emerge throughout the wrangling process. In the real world, such problems are often difficult or impossible to identify in advance. Such functions are therefore only useful in certain circumstances where the datasets involved are numerous and fairly similar in structure.

### Ethnicity

This next dataset follows a similar process. Data from the American Community Survey is very well maintained and has technical documentation. The similarity in the wrangling process is a result of the similarity in the way the datasets are organized. 

Using a similar wrangling process, we make the column names more practical and then calculate predictors that will be used in our regression analysis.

```{r}
dim(ethnicity)

glimpse(ethnicity)

colnames(ethnicity)
colnames(ethnicity) <- ethnicity[1,]
ethnicity <- ethnicity[-1,]

class(ethnicity$`Estimate; Total:`)
class(ethnicity$`Estimate; Hispanic or Latino:`)

ethnicity <- ethnicity %>%
  rename("FIPS"=Id2,
         "Total"=`Estimate; Total:`,
         "Hispanic_Latino"=`Estimate; Hispanic or Latino:`) %>%
  mutate("Total"=as.numeric(Total),
         "Hispanic_Latino"=as.numeric(Hispanic_Latino)) %>%
  mutate(Perc_Hispanic_Latino=Hispanic_Latino/Total) %>%
  dplyr::select(FIPS,
         Perc_Hispanic_Latino)
```

### Median income

The next couple of datasets, while from the American Community Survey, would be even more difficult to wrangle effectively with a function. 

The wrangling process for this dataset, which will eventually represent the median income of each county, is much more complex. 

The dataset has `r dim(med_inc)[2]` columns. This is far too many columns to sift through efficiently or effectively. 

```{r}
dim(med_inc)
```

We need to learn a bit more about this dataset before we can continue. 

Like our prior ACS datasets, the first 50 columns appear to pertain to a codebook.

```{r}
colnames(med_inc)[1:50]
```

The first row appears to have more practical column names.

```{r}
head(med_inc)
```

We substitute the impractical column names with more practical column names. 

```{r}
colnames(med_inc) <- med_inc[1, ]
med_inc <- med_inc[-1,]
```

Now that we have practical column names, we can use regular expressions to search through the `dim(med_inc)[2]` columns. 

`str_detect()` returns a logical vector, with a value of `TRUE` being returned for specified patterns found within a string. `R` can sum these `TRUE` values. This sum gives a count of how many values satisfied the logical statement. 

We can use this to determine which patterns are most successful at narrowing down our search. 

By condititioning on two statements, we are able to reduce the `r dim(med_inc)[2]` columns to `r sum(str_detect(colnames(med_inc), "[Mm]edian [Ii]ncome") & str_detect(colnames(med_inc),"[Ee]stimate"))`. 

Below we used brackets around letters to denote that the pattern could include either a capital or lowercase letter. 

From the first 50 columns we looked at, it becomes apparent that there are three key words that may be particularly useful: [Mmedian], [Ii]ncome, and [Ee]stimate. We can use either of these words or condition on both to search for relevant columns. 

Let's count the number of column names that contain each patter. Eventually, we want to be able to extract those column names, meaning we want a number small enough to sift through but large enough to not be impractical.

```{r}
sum(str_detect(colnames(med_inc), "[Mm]edian [Ii]ncome"))
sum(str_detect(colnames(med_inc), "[Ee]stimate"))
sum(str_detect(colnames(med_inc), "[Mm]edian [Ii]ncome") &
      str_detect(colnames(med_inc),"[Ee]stimate"))
```

`r sum(str_detect(colnames(med_inc), "[Mm]edian [Ii]ncome") & str_detect(colnames(med_inc),"[Ee]stimate"))` columns is a promising amount of column names to sift through. We can determine what these`r sum(str_detect(colnames(med_inc), "[Mm]edian [Ii]ncome") & str_detect(colnames(med_inc),"[Ee]stimate"))` columns are using the `which()` function. 

```{r}
colnames(med_inc)[which(str_detect(colnames(med_inc), "[Mm]edian [Ii]ncome") & 
                          str_detect(colnames(med_inc), "[Ee]stimate"))]
```

From here we can determine which columns we need and proceed as we did with the prior datasets.

We will use the following column as a predictor. 

```{r}
colnames(med_inc)[which(str_detect(colnames(med_inc), "[Mm]edian [Ii]ncome") & 
                          str_detect(colnames(med_inc), "[Ee]stimate"))][1]
```

We can save the name of this column to an object and then use that object in conjunction with the `paste()` function in dplyr verbs.

```{r}
median_income_column <- colnames(med_inc)[which(str_detect(colnames(med_inc), "[Mm]edian [Ii]ncome") & 
                          str_detect(colnames(med_inc), "[Ee]stimate"))][1]

med_inc <- med_inc %>%
  rename("FIPS"=Id2,
         "median_income"=paste(median_income_column)) %>%
  dplyr::select(FIPS,
         median_income)
```

After checking the resulting class of our predictor, we coerce it to numeric class.

```{r}
class(med_inc$median_income)
med_inc$median_income <- as.numeric(med_inc$median_income)
```

### Percent insured

The last dataset we wrangle by the American Community Survey follows a similar process.  

After checking the dimensions of the dataset, we introduce more practical column names, use regular expressions to sift through numerous columns, and extract that column for use in our analysis. 

```{r}
dim(perc_insured)

colnames(perc_insured)[1:50]

head(perc_insured)[,1:5]

colnames(perc_insured) <- perc_insured[1, ]

perc_insured <- perc_insured[-1,]

sum(str_detect(colnames(perc_insured), "[Pp]ercent [Ii]nsured"))
sum(str_detect(colnames(perc_insured), "[Ee]stimate"))
sum(str_detect(colnames(perc_insured), "[Pp]ercent [Ii]nsured") &
      str_detect(colnames(perc_insured),"[Ee]stimate"))

colnames(perc_insured)[which(str_detect(colnames(perc_insured), "[Pp]ercent [Ii]nsured") & 
                          str_detect(colnames(perc_insured), "[Ee]stimate"))]

percent_insured_column <- colnames(perc_insured)[which(str_detect(colnames(perc_insured), "[Pp]ercent [Ii]nsured") & 
                          str_detect(colnames(perc_insured), "[Ee]stimate"))][1]

perc_insured <- perc_insured %>%
  rename("FIPS"=Id2,
         "perc_insured"=paste(percent_insured_column)) %>%
  dplyr::select(FIPS,
         perc_insured)

class(perc_insured$perc_insured)
perc_insured$perc_insured <- as.numeric(perc_insured$perc_insured)
```

## Underlying cause of death data

Let's wrangle the mortality data. 

```{r}
dim(ucd_heart)

glimpse(ucd_heart)
```

The crude rate included in the mortality data is calculated by dividing the number of deaths by a population that differs from that of the actual population of each county. For crude rates, the Centers for Disease Control exludes certain members of the population. We will be including all members of the population as an offset in our regression analysis using data from the American Community Survey, meaning we can get rid of the population estimates and crude rates provided in the dataset.

**MICHAEL** Must provide explanation for why `NULL` is used here.

```{r}
ucd_heart$Notes <- NULL
ucd_heart$Population <- NULL
ucd_heart$`Crude Rate` <- NULL
ucd_heart$`2013 Urbanization Code` <- NULL
```

We then coerce the urbanization categories to a `factor` class. 

```{r}

class(ucd_heart$`2013 Urbanization`)
ucd_heart$`2013 Urbanization` <- as.factor(ucd_heart$`2013 Urbanization`)
glimpse(ucd_heart)
```

*According to [this](https://www.cdc.gov/nchs/data/series/sr_02/sr02_166.pdf) document, the ordering should be something along these lines*

We rename the five digit reference codes and order the urbanization categories. 

```{r}
ucd_heart <- ucd_heart %>%
  rename("FIPS"=`County Code`)

levels(ucd_heart$`2013 Urbanization`)
ucd_heart$`2013 Urbanization` <- factor(ucd_heart$`2013 Urbanization`,
       levels = c("NonCore (Nonmetro)",
                  "Micropolitan (Nonmetro)",
                  "Small Metro",
                  "Medium Metro",
                  "Large Fringe Metro",
                  "Large Central Metro"),
       ordered = TRUE)

levels(ucd_heart$`2013 Urbanization`)
```

We then collapse the categories we created. This will allow us visualize the categories in another way should the original categories be too noisy. 

```{r}
ucd_heart <- ucd_heart %>% mutate("2013 Urbanization Simplified"=fct_collapse(`2013 Urbanization`,
               large_metro=c("Large Fringe Metro",
                  "Large Central Metro"),
               small_medium_metro=c("Small Metro",
                  "Medium Metro"),
               non_metro=c("NonCore (Nonmetro)",
                  "Micropolitan (Nonmetro)")))
```

We check for any missing observations. 

```{r}
apply(ucd_heart, 2, function(x)sum(is.na(x)))
```

It appears that there may be 78 rows with missing information. After confirming this, we discard these rows.

```{r}
ucd_heart %>%
  filter(is.na(Deaths) & is.na(FIPS))

ucd_heart <- ucd_heart %>%
  filter(!(is.na(Deaths) & is.na(FIPS)))
```

## United States county boundaries

Recall that we transformed the spatial data as soon as the data was imported. This is considered good practice. Depending on the projection of the spatial data, not doing so could have unintended consequences on the analysis.

We transformed the spatial data to the *US National Atlas Equal Area* projection. This projection does a good job of representing the size of states. Some projections, such as the popular Mercator projection, make areas near the north and south poles appear disproportionately larger than areas further away. In the Mercator projection, Greenland is about the same size as Africa, when the latter is in fact larger the former. 

We use the `st_crs` function to determine the projection (coordinate reference system) used by the United States county boundaries imported. We set the projection immediately after importing the boundaries to the **US National Atlas Equal Area** projection, identifiable by an EPSG of 2163.

```{r}
st_crs(map_sf)
```

```{r}
dim(map_sf)

glimpse(map_sf)
```

```{r}
map_sf <- map_sf %>%
  rename("FIPS"=GEOID) %>%
  left_join(ucd_heart, by="FIPS")

class(map_sf$Deaths)
class(map_sf$`2013 Urbanization`)
```

```{r}
glimpse(FIPS_codes)

head(FIPS_codes)

colnames(FIPS_codes)
FIPS_codes[4,1:6]
colnames(FIPS_codes) <- FIPS_codes[4,]
FIPS_codes <- FIPS_codes[-(1:4),]

glimpse(FIPS_codes)

sapply(FIPS_codes[,1:(length(colnames(FIPS_codes)))], class) # Apply this to other section of the code where checking of column class is necessary. 

FIPS_codes <- sapply(FIPS_codes[,1:length(colnames(FIPS_codes))], as.factor)

class(FIPS_codes)

FIPS_codes <- as.data.frame(FIPS_codes)

sapply(FIPS_codes[,1:(length(colnames(FIPS_codes))-1)], summary)

colnames(FIPS_codes)

FIPS_codes <- FIPS_codes %>%
  dplyr::select(`Summary Level`,
         `State Code (FIPS)`,
         `County Code (FIPS)`,
         `Area Name (including legal/statistical area description)`) %>%
  rename("summary_level"=`Summary Level`,
         "state_code"=`State Code (FIPS)`,
         "county_code"=`County Code (FIPS)`,
         "area_name"=`Area Name (including legal/statistical area description)`) %>%
  sapply(as.character) %>%
  as.data.frame() %>% #Discuss how using sapply converted our dataframe into a matrix, what we are doing essentially is turning it back into a dataframe for manipulation
  mutate("FIPS"=paste0(state_code,county_code))

FIPS_codes_dfs <- FIPS_codes %>%
  group_split(summary_level)

FIPS_codes_dfs %>%
  lapply(`[`, 1, )

state_codes <- FIPS_codes_dfs[[2]]
glimpse(state_codes)
state_codes <- state_codes %>%
  dplyr::select(state_code, area_name) %>%
  rename("state_name"=area_name)
glimpse(state_codes)
county_codes <- FIPS_codes_dfs[[3]]
glimpse(county_codes)
county_codes <- county_codes %>%
  dplyr::select(-summary_level) %>%
  rename("county_name"=area_name)

# Mention how there is a heirarchy to these codes 

covariates <- county_codes %>%
  left_join(state_codes, by="state_code") %>%
  left_join(total_pop, by="FIPS") %>%
  left_join(med_age, by="FIPS") %>%
  left_join(race, by="FIPS") %>%
  left_join(ethnicity, by="FIPS") %>%
  left_join(med_inc, by="FIPS") %>%
  left_join(perc_insured, by="FIPS")
```

*Now that we have the reference codes, we can update this section*

We can use information from the US Census Bureau to identify which codes correspond to the states of interest (click. [here](https://www.census.gov/geographies/reference-files/2017/demo/popest/2017-fips.html)).

```{r}
map_sf <- map_sf %>%
  left_join(covariates, by="FIPS")
```

```{r}
# Using "" around numbers to prevent 0 from disappearing from "09" later in the analysis. If the zero disappears, R does not recognize CT in maps or incorrectly classifies it later in the analysis. 

state_fp <- c("39", #OH
              "54", #WV
              "51", #VA
              "24", #MD
              "10", #DE
              "34", #NJ
              "42", #PA 
              "36", #NY
              "09", #CT
              "44", #RI
              "25", #MA
              "50", #VT
              "33", #NH
              "23" #ME
              )
```

```{r}
subset <- map_sf %>%
  filter(STATEFP %in% state_fp)
```

```{r}
contiguous_us <- map_sf %>% filter(!(STATEFP %in% c("02", #Alaska
                                                    "15", #Hawaii
                                                    "72" #Puerto Rico
                                                    )))
```

When using reference codes to join datasets, it's important to check for missing values. If the datasets use the same reference code system—but were updated at different times or use a slightly different convention—some observations will be unable to join correctly. This could cause issues that are difficult to diagnose later on. As always, it's important to check your work, especially early on during the wrangling process.

[The CDC has published a document with changes to county geographies ](https://www.cdc.gov/nchs/nvss/bridged_race/county_geography-_changes2015.pdf). If we open this document, we learn that some counties are referred to by names other than those listed in mortality files. 

Let's examine our dataset to see if this is an issue. 

We can do this by examining the number of missing values for a given column. This is because errors in joins often manifest themselves as missing values. 

Using the `apply()` function, we can count the number of missing values per column. It appears that there are two types of missing values: missing values caused by suppressed death counts and missing values pertaining to county names and/or urbanization categories. 

```{r}
apply(contiguous_us, 2, function(x)sum(is.na(x)))
```

If we look at the missing data more closely we find that the missing values pertaining to county names and/or urbanization categories is entirely attributable to one county. This county is listed as having had a change in name on the document published by the CDC referred to previously.

```{r}
contiguous_us %>%
  filter(is.na(County)|
           is.na(`2013 Urbanization`)) %>% 
  pull(NAME)
```

Furthermore, this observation has an observed population but missing death count and crude mortality rate. 

```{r}
contiguous_us %>%
  filter(is.na(County)|
           is.na(`2013 Urbanization`)) %>% 
  pull(Population)

contiguous_us %>%
  filter(is.na(County)|
           is.na(`2013 Urbanization`)) %>% 
  pull(Deaths)
```

Assuming that an overwhelming majority of missing mortality data is a result of data suppression, we can determine that there was a single error that occurred when joining datasets by reference codes given the above information. Additionally, because population data is properly joined and mortality data is missing we can deduce that this single error emerged when the mortality data was joined.

Indeed, if we look at the mortality data we find that what is now Oglala Lakota County (46201) is referred to as Shannon County (46113).  

```{r}
ucd_heart %>%
  filter(FIPS==46113|FIPS==46102)
```

To fix the error, we need to treat Oglala Lakota County (46102) as if it were Shannon County (46113) in what will become our analysis dataframe. 

To do this we overwrite data from Oglala Lakota County (46102) with data from Shannon County (46113). We create a dataframe with the data that we want to use to replace missing values. Then we use `left_join()` to merge this dataframe to what will become our analysis dataframe. This causes two additional columns to be made, each with the same name as the original column and with an `.x` or `.y` at the end. This occurs when joins are done with datasets that have similarly named columns.  We then use the `coalesce()` function to merge the two colums that were created during this join into one column. Lastly, we use the `select()` function in conjuction with the `names` function to ensure that we obtain a dataset with the same columns we started with.

```{r}
oglala_correction <- ucd_heart %>%
  filter(FIPS=="46113") %>%
  mutate(FIPS="46102", County="Oglala Lakota")

contiguous_us <- contiguous_us %>%
  left_join(oglala_correction, by="FIPS") %>%
  mutate(Deaths=coalesce(Deaths.x, Deaths.y)) %>%
  mutate(County=coalesce(`County.x`,`County.y`)) %>%
  mutate(`2013 Urbanization`=coalesce(`2013 Urbanization.x`,`2013 Urbanization.y`)) %>%
  mutate(`2013 Urbanization Simplified`=coalesce(`2013 Urbanization Simplified.x`,`2013 Urbanization Simplified.y`)) %>%
  select(names(contiguous_us))
```

Now that we have the death count and population of each county correctly joined, we can calculated the crude rate per 100,000 people.

```{r}
contiguous_us <- contiguous_us %>%
  mutate("Crude Rate"=round((Deaths/Population)*1e+05,3))  %>%
  mutate("Crude Rate"=round((Deaths/Population)*1e+05,3))
```

## Lag Covariates

We need to construct the neighborhood matrix in a way that does includes all the relationships between counties of interest, otherwise we will lose valuable information and could bias our results.

Let's create a neighborhood matrix that includes counties just outside of our region of interest. 

Five states border our region of interest. Let's include them in our neighborhood matrix.

```{r}
us_cvd_sp_data <- contiguous_us
```

```{r, eval=FALSE}
ucd_cvd_sp_data <- us_cvd_sp_data %>%
  filter(!is.na(Deaths))
```

```{r}
# Would be neat to give an example of queen's vs rook's case matrices
us_cvd_sp_data_sp <- as(us_cvd_sp_data, 'Spatial')
nbrs_us_cvd_sp_data_sp <- poly2nb(us_cvd_sp_data_sp, queen = TRUE)
neighbors_sf <- as(nb2lines(nbrs_us_cvd_sp_data_sp, coords = coordinates(us_cvd_sp_data_sp)), 'sf')
neighbors_sf <- st_set_crs(neighbors_sf, st_crs(us_cvd_sp_data))
```

```{r}
ggplot(us_cvd_sp_data) +
  geom_sf(data = neighbors_sf, size=0.25, color="black") +
  ggtitle("Neighborhood Matrix", subtitle = "Queen's Case Adjacency") +
  theme_minimal()
```

#### Smoothing

With this matrix, we can create smoothed maps. Smoothing is the process of borrowing information from surrounding area-level units to create values with less variation.

Our regression will require that we create lagged variables, variables with values that are summaries of proximal regions. What we consider proximal is a matter of our analysis question.

Recall our adjacency matrix. We can borrow information from area-level units—in our case, counties—that are queen adjacent. Doing this would be considered smoothing.

If we smoothed nearest queen adjacent area-level units, we would be creating a first order lagged variable. 

Below we create a first order lagged variable.

```{r}
# First order lag

FFct_nb_w <- nb2listw(nbrs_us_cvd_sp_data_sp,style="W", zero.policy = TRUE)
Wfull<-listw2mat(FFct_nb_w)

us_cvd_sp_data$smoothed_pop <- Wfull %*% us_cvd_sp_data$Population

ggplot(us_cvd_sp_data) +
  geom_sf(aes(fill=smoothed_pop)) +
  scale_fill_gradientn(colours = viridis(10))
```

If we smoothed using the next nearest adjacent area-level units, we would be creating a second order lagged variable. 

The process for creating a second order lagged variable, and so on, is the same.

```{r}
# Second order lag 

nb_sec_lag_list <- nblag(nbrs_us_cvd_sp_data_sp, maxlag = 2)
nb_sec_lag <- nb_sec_lag_list[[2]]

FFct_nb_w <- nb2listw(nb_sec_lag,style="W", zero.policy = TRUE)
Wfull<-listw2mat(FFct_nb_w)

us_cvd_sp_data$smoothed_pop_sec_order <- Wfull %*% us_cvd_sp_data$Population

ggplot(us_cvd_sp_data) +
  geom_sf(aes(fill=smoothed_pop_sec_order)) +
  scale_fill_gradientn(colours = viridis(10))
```

We can also combine combine information from the nearest and next nearest adjacent area-level units. 

The code below uses the list of first and second order adjacent counties to do exactly that. 
```{r}
nb_cum_lag <- nblag_cumul(nb_sec_lag_list)

FFct_nb_w <- nb2listw(nb_cum_lag,style="W", zero.policy = TRUE)
Wfull<-listw2mat(FFct_nb_w)

us_cvd_sp_data$smoothed_pop_cum_order <- Wfull %*% us_cvd_sp_data$Population

ggplot(us_cvd_sp_data) +
  geom_sf(aes(fill=smoothed_pop_cum_order)) +
  scale_fill_gradientn(colours = viridis(10))
```

If we take an extreme case, we can better visualize the implications such smoothing can have on our data. 

Below, we take an average of the first through tenth order adjacent counties. 

The end product is a map with much less variation. 

```{r}
nb_ext_lag_list <- nblag(nbrs_us_cvd_sp_data_sp, maxlag = 10)
nb_ext_cum_lag <- nblag_cumul(nb_ext_lag_list)

FFct_nb_w <- nb2listw(nb_ext_cum_lag,style="W", zero.policy = TRUE)
Wfull<-listw2mat(FFct_nb_w)

us_cvd_sp_data$smoothed_pop_ext_cum_order <- Wfull %*% us_cvd_sp_data$Population

ggplot(us_cvd_sp_data) +
  geom_sf(aes(fill=smoothed_pop_ext_cum_order)) +
  scale_fill_gradientn(colours = viridis(10))
```

If we plot the difference of the smoothed population from the actual population, we see that there is systematic overestimation and underestimation throughout this map. This is because of the effect smoothing has on our values. The more counties we use to generate an average summary value for a county, the more the summary value is alike to the average of the entire region. The reduced variation has both a cost and a benefit. 

```{r}
us_cvd_sp_data <- us_cvd_sp_data %>%
  mutate(Bias = Population - smoothed_pop_ext_cum_order) %>%
  mutate(Bias_cat = cut(Bias, breaks = 7, include.lowest = TRUE, ordered_result = TRUE)) %>%
  mutate("Bias Class" = ifelse(sign(Bias)==-1, "Overestimation", ifelse(sign(Bias)==1, "Underestimation", "Neither"))) 

summary(us_cvd_sp_data$Bias)

ggplot(us_cvd_sp_data) +
  geom_sf(aes(fill=Bias_cat)) +
  scale_fill_manual(values=c(viridis(7)))

ggplot(us_cvd_sp_data) +
  geom_sf(aes(fill=`Bias Class`)) +
  scale_fill_manual(values = c(viridis(2)))
```

We can think of this in a different way. What if we were instead analyzing data for the entire contiguous US? If we smoothed enough so that the entire northeast region had a very similar population per county, the county values would be very similar to the average of the region. This idea still holds when we are analyzing a smaller region: if we smooth values enough, the values we produce tend to reflect the average of the region we our analyzing.

We now need to develop an object so that, with matrix multiplication, we can compute the arithmetic mean of nearby counties. 

We won't be able to compute the arithmetic mean with missing values via matrix multiplication. We check below for missing values 
```{r}
sum(is.na(contiguous_us$Population))
```

```{r}
contiguous_us_sp <- as(contiguous_us, 'Spatial')
nbrs_contiguous_us_sp <- poly2nb(contiguous_us_sp, queen = TRUE)

nb_ext_lag_list_contiguous <- nblag(nbrs_contiguous_us_sp, maxlag = 10)
nb_ext_cum_lag_contiguous <- nblag_cumul(nb_ext_lag_list_contiguous)

A <- nb2listw(nb_ext_cum_lag_contiguous,style="W", zero.policy = TRUE)
Wfulle <-listw2mat(A)

contiguous_us$smoothed_pop_ext_cum_order <- Wfulle %*% contiguous_us$Population

ggplot(contiguous_us) +
  geom_sf(aes(fill=smoothed_pop_ext_cum_order), color="transparent") + 
  scale_fill_gradientn(colors = viridis(10))
```

Broadly speaking, smoothing makes values look more like the global average, thereby reducing the global variance, exchanging exactitude in a map for the easier visualization. Smoothing creates fictitious values that are more than often overestimates or understimates of the true value. Despite this, we can use smoothing to advance our analysis with proper caution. Smoothing can help produce more interpretable visualizations as well as lagged variables that can be used in both inferential and predictive models. 

Using the code we used earlier, we can create a function that produces a lagged covariate. This is more efficient that manually creating each variable with the same code and prevents errors. Conversely, errors in our function can be systematically applied to each of our covariates. Since we already have the code and are assured that it functions as we intend, this is not a problem.

```{r}
FFct_nb_w <- nb2listw(nbrs_us_cvd_sp_data_sp,style="W", zero.policy = TRUE)
Wfull<-listw2mat(FFct_nb_w)

lag_creator <- function(vector){
  lag_vector <- Wfull %*% vector
  lag_vector
}

lag_creator_plus <- function(vector, lags=2){
  lag_order_list <- nblag(nbrs_us_cvd_sp_data_sp, maxlag = lags)
  nb_cum_lag <- nblag_cumul(lag_order_list)
  lag_vector <- Wfull %*% vector
  lag_vector
}

sum(complete.cases(covariates)==FALSE)

covariates[which(complete.cases(covariates)==FALSE),]

us_cvd_sp_data <- us_cvd_sp_data %>%
  mutate("median_age_lag"=lag_creator(median_age),
         "Perc_W_only_lag"=lag_creator(Perc_W_only),
         "Perc_BA_only_lag"=lag_creator(Perc_BA_only),
         "Perc_A_only_lag"=lag_creator(Perc_A_only),
         "Perc_Hispanic_Latino_lag"=lag_creator(Perc_Hispanic_Latino),
         "median_income_lag"=lag_creator(median_income),
         "perc_insured_lag"=lag_creator(perc_insured),
         "Population_lag"=lag_creator(Population)
         )

us_cvd_sp_data <- us_cvd_sp_data %>%
  mutate("median_age_lag2"=lag_creator_plus(median_age),
         "Perc_W_only_lag2"=lag_creator_plus(Perc_W_only),
         "Perc_BA_only_lag2"=lag_creator_plus(Perc_BA_only),
         "Perc_A_only_lag2"=lag_creator_plus(Perc_A_only),
         "Perc_Hispanic_Latino_lag2"=lag_creator_plus(Perc_Hispanic_Latino),
         "median_income_lag2"=lag_creator_plus(median_income),
         "perc_insured_lag2"=lag_creator_plus(perc_insured),
         "Population_lag2"=lag_creator_plus(Population)
         )

```

# Exploratory data analysis 

Before we analyze any of our data, let's set a randomization seed. This will make markov chain estimates reproducible. 

```{r}
set.seed(9999)
```

We can explore our data in different ways now that we've merged our data to one of the maps we've imported. 

The great thing about using sf objects is that they work very well with the `tidyverse` and `ggplot` packages. The code required to visualize and summarize our spatial data is extremely similar, if not identical to that used in non-spatial applications. 

Let's create a map of the crude mortality rate of cardiovascular diseases across the contiguous United States to help identify trends beyond our region of interest. 

It appears that rural regions of the United States may have greater crude mortality rates. 

```{r}
ggplot(contiguous_us) +
  geom_sf(aes(fill=log(`Crude Rate`)), color="black", lwd=0.1) +
  scale_fill_gradientn(colours = viridis(5)) +
  labs(title = "Crude Rate", subtitle = "Contiguous United States", caption = "Calculated with UCD data from CDC WONDER") +
  theme_OCS
```

**MICHAEL** This needs to be included in the data wrangling portion. 

We need to create one last covariate representing the distance from the nearest large central metropolitan county in 10 km units. 

As done so previously, we check the projection of the spatial data we are working with. 

```{r}
st_crs(us_cvd_sp_data)
```

We then create an dataframe of the large central metropolitan counties. 

```{r}
LC_metro1 <- us_cvd_sp_data %>% filter(`2013 Urbanization`=="Large Central Metro") %>% mutate(index=row_number())

st_agr(us_cvd_sp_data) <- "constant"

st_agr(LC_metro1) <- "constant"
```

We want to compute the shortest length between each county, the curve of which on a sphere is referred to as a geodesic. The length of the geodesic is also known as great circle distance. To calculate great circle distance, we need to transform our dataframe from a projected coordinate system based on a two-dimensional plane back to a geographic coordinate system based on the angles produced relative to earth's center. Additionally, we need to obtain the centroids of each county to calculate this distance. We can calculate the distance between boundary lines, but this would make it difficult to differentiate the distance between a county and itself from and the distance between a county and its neighbor. For this analysis, we will use county centroids to make this distinction. In a separate analysis, it may be more appropriate to calculate the distance between boundary lines.

In the code below we transform both the spatial data with all counties and the spatial data with large metropolitan counties to the **World Geodetic System, 1984** coordinate system. We use the `st_centroid()` function by `sf` to obtain the centroids of each area-unit. 

```{r}
us_cvd_sp_data_geodetic <- st_transform(st_centroid(us_cvd_sp_data), 4326)

dim(us_cvd_sp_data_geodetic)

LC_metro1_geodetic <- st_transform(st_centroid(LC_metro1), 4326)

dim(LC_metro1_geodetic)
```

With then then calculate the distance between the centroids of all counties and large metropolitan counties using the `st_distance()` function by `sf`. Note that we specify that the distance calculated be great circle distance. 

```{r}
cen_dist_matrix <- st_distance(x=us_cvd_sp_data_geodetic, y=LC_metro1_geodetic, which = "Great Circle")
```


**MICHAEL** This needs to be finished. 

The resulting object has `r dim(cen_dist_matrix)[1]` rows and `r dim(cen_dist_matrix)[2]` columns representing all counties and large metropolitan counties, respectively.

```{r}
dim(cen_dist_matrix)
```

We wanted to obtain the shorted

```{r}
us_cvd_sp_data <- us_cvd_sp_data %>% mutate(cen_dist_m=apply(data.frame(cen_dist_matrix), 1,min),
                         index=apply(cen_dist_matrix, 1, function(matrix_row) which(matrix_row==min(matrix_row)))) %>%
  mutate(cen_dist_10km=cen_dist_m/(1000*10))

LC_metro1 <- LC_metro1 %>% as.data.frame() %>% dplyr::select(index, county_name) %>% rename("nearest_LC_metro"=county_name)

us_cvd_sp_data <- us_cvd_sp_data %>% left_join(LC_metro1, by="index")
```

*eval=FALSE*
us_cvd_sp_data <- us_cvd_sp_data %>% mutate(index=st_nearest_feature(st_centroid(us_cvd_sp_data), st_centroid(LC_metro1)))
*chunk ends*

```{r}
# "" around numbers for the same reason as mentioned before

surrounding_states <- c("37", #NC
                        "47", #TN
                        "21", #KY
                        "18", #IN
                        "26" #MI
                        )

matrix_counties <- c(state_fp, surrounding_states)

NE_cvd_sp_data <- us_cvd_sp_data %>% 
  filter(STATEFP %in% matrix_counties)

NE_cvd_sp_data <- NE_cvd_sp_data %>%
  mutate(Region = ifelse(STATEFP %in% state_fp, "Region of Interest", "Surrounding States"))

```

Let's look at what we did visually. 

```{r}
ggplot(NE_cvd_sp_data) + 
  geom_sf(aes(fill=Region))
```

```{r}
mortality_missing <- us_cvd_sp_data %>% filter(is.na(Deaths))

mortality_complete <- us_cvd_sp_data %>% filter(!is.na(Deaths))

st_agr(mortality_missing) <- "constant"

st_agr(mortality_complete) <- "constant"

mortality_missing_geodetic <- st_transform(st_centroid(mortality_missing), 4326)

mortality_complete_geodetic <- st_transform(st_centroid(mortality_complete), 4326)

missing_mortality_matrix <- st_distance(x=mortality_missing_geodetic, y=mortality_complete_geodetic, which = "Great Circle")

missing_nearest_neighbors <- t(apply(missing_mortality_matrix, 1, order)[1:5, ])

dim(missing_nearest_neighbors)

missing_nearest_neighbors <- as.data.frame(missing_nearest_neighbors)

mortality_complete_no_geom <- mortality_complete

st_geometry(mortality_complete_no_geom) <- NULL

missing_nearest_neighbors <- apply(missing_nearest_neighbors, 1,function(x)(mortality_complete_no_geom[x,"Crude Rate"]))

missing_nearest_neighbors <- sapply(missing_nearest_neighbors,"[[", "Crude Rate")

dim(missing_nearest_neighbors)

missing_nearest_neighbors <- t(missing_nearest_neighbors)

mortality_missing <- mortality_missing %>%
  mutate("Crude Rate" = apply(missing_nearest_neighbors ,1, mean),
         Deaths = ifelse(is.na(Deaths), round((`Crude Rate` * Population)/1e+05, 0), Deaths)) 

us_cvd_sp_data <- mortality_complete %>%
  rbind(mortality_missing)

apply(us_cvd_sp_data, 2, function(x)sum(is.na(x)))

us_cvd_sp_data[which(is.na(us_cvd_sp_data$Deaths)),]
```

```{r}
ggplot(us_cvd_sp_data) +
  geom_sf(aes(fill=Deaths), color="black", lwd=0.1) +
  scale_fill_gradientn(colours = viridis(5)) +
  labs(title = "Deaths", subtitle = "Contiguous United States", caption = "Calculated with UCD data from CDC WONDER") +
  theme_OCS
```

```{r}
ggplot(us_cvd_sp_data) +
  geom_sf(aes(fill=log(Deaths+1)), color="black", lwd=0.1) +
  scale_fill_gradientn(colours = viridis(5)) +
  labs(title = "log(Deaths+1)", subtitle = "Contiguous United States", caption = "Calculated with UCD data from CDC WONDER") +
  theme_OCS
```

```{r}
ggplot(us_cvd_sp_data) +
  geom_sf(aes(fill=Population), color="black", lwd=0.1) +
  scale_fill_gradientn(colours = viridis(5)) +
  labs(title = "Population", subtitle = "Contiguous United States", caption = "Calculated with UCD data from CDC WONDER") +
  theme_OCS
```

```{r}
ggplot(us_cvd_sp_data) +
  geom_sf(aes(fill=log(Population)), color="black", lwd=0.1) +
  scale_fill_gradientn(colours = viridis(5)) +
  labs(title = "log(Population)", subtitle = "Contiguous United States", caption = "Calculated with UCD data from CDC WONDER") +
  theme_OCS
```

```{r}
ggplot(us_cvd_sp_data) +
  geom_sf(aes(fill=log(`Crude Rate`)), color="black", lwd=0.1) +
  scale_fill_gradientn(colours = viridis(5)) +
  labs(title = "Crude Rate", subtitle = "Contiguous United States", caption = "Calculated with UCD data from CDC WONDER") +
  theme_OCS
```

```{r}
ggplot(us_cvd_sp_data) +
  geom_sf(aes(fill=`2013 Urbanization`), color="black", lwd=0.1) +
  labs(title = "2013 Urbanization", subtitle = "Contiguous United States", caption = "Calculated with UCD data from CDC WONDER") +
  theme_OCS
```

```{r}
ggplot(us_cvd_sp_data) +
  geom_sf(aes(fill=Perc_W_only), color="black", lwd=0.1) +
  scale_fill_gradientn(colours = viridis(5)) +
  labs(title = "Perc_W_only", subtitle = "Contiguous United States", caption = "Calculated with UCD data from CDC WONDER") +
  theme_OCS
```

```{r}
ggplot(us_cvd_sp_data) +
  geom_sf(aes(fill=Perc_BA_only), color="black", lwd=0.1) +
  scale_fill_gradientn(colours = viridis(5)) +
  labs(title = "Perc_B_only", subtitle = "Contiguous United States", caption = "Calculated with UCD data from CDC WONDER") +
  theme_OCS
```

```{r}
ggplot(us_cvd_sp_data) +
  geom_sf(aes(fill=Perc_A_only), color="black", lwd=0.1) +
  scale_fill_gradientn(colours = viridis(5)) +
  labs(title = "Perc_A_only", subtitle = "Contiguous United States", caption = "Calculated with UCD data from CDC WONDER") +
  theme_OCS
```

```{r}
ggplot(us_cvd_sp_data) +
  geom_sf(aes(fill=Perc_Hispanic_Latino), color="black", lwd=0.1) +
  scale_fill_gradientn(colours = viridis(5)) +
  labs(title = "Perc_Hispanic_Latino", subtitle = "Contiguous United States", caption = "Calculated with UCD data from CDC WONDER") +
  theme_OCS
```

```{r}
ggplot(us_cvd_sp_data) +
  geom_sf(aes(fill=perc_insured), color="black", lwd=0.1) +
  scale_fill_gradientn(colours = viridis(5)) +
  labs(title = "perc_insured", subtitle = "Contiguous United States", caption = "Calculated with UCD data from CDC WONDER") +
  theme_OCS
```

# Data analysis 

```{r}
model0 <- glm(Deaths ~ 1, 
            family="poisson",
            offset = log(Population),
            data=us_cvd_sp_data,
            na.action = na.exclude)

us_cvd_sp_data <- us_cvd_sp_data %>%
  mutate("residuals0"=residuals.glm(model0, "response"))

us_cvd_sp_data %>%
  ggplot() +
  geom_sf(aes(fill=residuals0), color="transparent") + 
  scale_fill_gradient2(low="green", mid="white", high="red")

us_cvd_sp_data <- us_cvd_sp_data[is.na(us_cvd_sp_data$residuals0)==FALSE,]
nbrs_us_cvd_sp_data_sp <- poly2nb(us_cvd_sp_data, queen = TRUE)
FFct_nb_w_test <- nb2listw(nbrs_us_cvd_sp_data_sp,style="W", zero.policy = TRUE)

moran_cor_m0 <-sp.correlogram(neighbours=FFct_nb_w_test$neighbours,var=us_cvd_sp_data$residuals0,order=5,method="I",style="W", zero.policy = TRUE)
```

```{r}
model1 <- glm(Deaths ~ median_age + 
              Perc_W_only +
              Perc_BA_only + 
              Perc_A_only +
              Perc_Hispanic_Latino + 
              median_income +
              perc_insured + 
              `2013 Urbanization`, 
            family="poisson",
            offset = log(Population),
            data=us_cvd_sp_data,
            na.action = na.exclude)

model2 <- glm(Deaths ~ median_age + 
              median_age_lag + 
              Perc_W_only + 
              Perc_W_only_lag +
              Perc_BA_only + 
              Perc_BA_only_lag + 
              Perc_A_only +
              Perc_A_only_lag + 
              Perc_Hispanic_Latino + 
              Perc_Hispanic_Latino_lag + 
              median_income +
              median_income_lag +
              perc_insured +
              perc_insured_lag + 
              `2013 Urbanization`, 
            family="poisson",
            offset = log(Population),
            data=us_cvd_sp_data,
            na.action = na.exclude)
```

The presence of multicollinearity means that there is correlation between variables. However, high correlation does not mean that there is multicollinearity. 

Let's produce some correlation matrices and compare them with computed variance inflation factors for our variables. 

```{r}
melt_cor1 <- as.data.frame(us_cvd_sp_data) %>%
  dplyr::select(median_age,
         Perc_W_only,
         Perc_BA_only,
         Perc_A_only,
         Perc_Hispanic_Latino,
         median_income,
         perc_insured) %>%
  cor() %>%
  round(3) %>%
  reshape2::melt()

colnames(melt_cor1)

melt_cor1 %>%
  ggplot(aes(x=Var1, y=Var2, fill=value, label=value)) +
  geom_tile() +
  scale_fill_gradient2(low="red", mid="white", high="green") + 
  geom_text() + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
        axis.text.y = element_text(angle = 45, vjust = 1, hjust = 1)) +
  ggtitle("Correlation matrix, focal effects") + 
  xlab("") +
  ylab("")
```

```{r}
melt_cor2 <- as.data.frame(us_cvd_sp_data) %>%
  dplyr::select(median_age_lag,
         Perc_W_only_lag,
         Perc_BA_only_lag,
         Perc_A_only_lag,
         Perc_Hispanic_Latino_lag,
         median_income_lag,
         perc_insured_lag) %>%
  cor() %>%
  round(3) %>%
  reshape2::melt()

colnames(melt_cor2)

melt_cor2 %>%
  ggplot(aes(x=Var1, y=Var2, fill=value, label=value)) +
  geom_tile() +
  scale_fill_gradient2(low="red", mid="white", high="green") + 
  geom_text() + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
        axis.text.y = element_text(angle = 45, vjust = 1, hjust = 1)) +
  ggtitle("Correlation matrix, lagged effects") + 
  xlab("") +
  ylab("")
```

```{r}
melt_cor3 <- as.data.frame(us_cvd_sp_data) %>%
  dplyr::select(median_age,
         Perc_W_only,
         Perc_BA_only,
         Perc_A_only,
         Perc_Hispanic_Latino,
         median_income,
         perc_insured,
         median_age_lag,
         Perc_W_only_lag,
         Perc_BA_only_lag,
         Perc_A_only_lag,
         Perc_Hispanic_Latino_lag,
         median_income_lag,
         perc_insured_lag) %>%
  cor() %>%
  round(3) %>%
  reshape2::melt()

colnames(melt_cor3)

melt_cor3 %>%
  ggplot(aes(x=Var1, y=Var2, fill=value, label=value)) +
  geom_tile() +
  scale_fill_gradient2(low="red", mid="white", high="green") + 
  geom_text(size=2) + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
        axis.text.y = element_text(angle = 45, vjust = 1, hjust = 1)) +
  ggtitle("Correlation matrix, focal/lagged effects") + 
  xlab("") +
  ylab("")
```

```{r}
vif(model1)
vif(model2)
```

```{r}
sum(is.na(us_cvd_sp_data$Deaths))
length(model1$residuals)
length(model2$residuals)
```

```{r}
#Are the residuals being plotted correctly? 

us_cvd_sp_data <- us_cvd_sp_data %>%
  mutate("residuals1"=residuals.glm(model1, "response"))

us_cvd_sp_data %>%
  ggplot() +
  geom_sf(aes(fill=residuals1), color="transparent") + 
  scale_fill_gradient2(low="green", mid="white", high="red")

us_cvd_sp_data <- us_cvd_sp_data %>%
  mutate("residuals2"=residuals.glm(model2, "response"))

us_cvd_sp_data %>% 
  ggplot() +
  geom_sf(aes(fill=residuals2), color="transparent") + 
  scale_fill_gradient2(low="green", mid="white", high="red")

moran_cor_m1 <-sp.correlogram(neighbours=FFct_nb_w_test$neighbours,var=us_cvd_sp_data$residuals1,order=5,method="I",style="W", zero.policy = TRUE)
moran_cor_m2 <-sp.correlogram(neighbours=FFct_nb_w_test$neighbours,var=us_cvd_sp_data$residuals2,order=5,method="I",style="W", zero.policy = TRUE)
plot(moran_cor_m1,ylim=c(-0.2, 0.4),main="Residuals from model 1")
plot(moran_cor_m2,ylim=c(-0.2, 0.4),main="Residuals from model 2")
```

us_cvd_sp_data <- us_cvd_sp_data[is.na(us_cvd_sp_data$residuals1)==FALSE,]
nbrs_us_cvd_sp_data_sp <- poly2nb(us_cvd_sp_data, queen = TRUE)
FFct_nb_w_test <- nb2listw(nbrs_us_cvd_sp_data_sp,style="W", zero.policy = TRUE)

Checking to see if average of neighbors is used. *This could be a great way to explain the weighted matrix. The code used in this case study appears somewhat black box-ish. It's important to not only describe the programming, but what it's actually doing.*

*This code does not work after the inclusion of code that removes surrounding states from the spatial data and was made no longer a chunk. An adaptation will be very helpful.*

us_cvd_sp_data <- us_cvd_sp_data %>% mutate(ID=row_number())
us_cvd_sp_data %>% filter(GEOID==36047) %>% pull(median_income_lag)
us_cvd_sp_data %>% filter(GEOID==36047) %>% pull(ID)
us_cvd_sp_data %>% filter(ID %in% FFct_nb_w_test[[2]][[773]]) %>% dplyr::select(median_income, GEOID) %>% summarise(average=mean(median_income))


FFct_nb_w_test[[2]][[773]]
#Code checks out! 

```{r}
chi_square_model1 = sum(residuals(model1, type = "pearson")^2)

degrees_freedom_model1 = model1$df.residual

p_value_model1 = 1-pchisq(chi_square_model1, degrees_freedom_model1)

paste("Pearson goodness of fit = ", round(chi_square_model1,2), ", df = ", degrees_freedom_model1, ", p-value = ", round(p_value_model1,10))
```

```{r}
chi_square_model2 = sum(residuals(model2, type = "pearson")^2)

degrees_freedom_model2 = model1$df.residual

p_value_model2 = 1-pchisq(chi_square_model2, degrees_freedom_model2)

paste("Pearson goodness of fit = ", round(chi_square_model2,2), ", df = ", degrees_freedom_model2, ", p-value = ", round(p_value_model2,10))
```

Both models show overdispersion. 

```{r}
model1_nb <- glm.nb(Deaths ~ median_age +
                      Perc_W_only +
                      Perc_BA_only +
                      Perc_A_only + 
                      Perc_Hispanic_Latino + 
              median_income +
              perc_insured + 
              `2013 Urbanization` + 
              offset(log(Population)), 
            data=us_cvd_sp_data,
            na.action = na.exclude)

model2_nb <- glm.nb(Deaths ~ median_age + 
              median_age_lag + 
              Perc_W_only + 
              Perc_W_only_lag +
              Perc_BA_only + 
              Perc_BA_only_lag + 
              Perc_A_only +
              Perc_A_only_lag + 
              Perc_Hispanic_Latino + 
              Perc_Hispanic_Latino_lag + 
              median_income +
              median_income_lag +
              perc_insured +
              perc_insured_lag +
              `2013 Urbanization` +
              offset(log(Population)),
            data=us_cvd_sp_data,
            na.action = na.exclude)
```

```{r}
vif(model1_nb)
vif(model2_nb)
```

```{r}
us_cvd_sp_data <- us_cvd_sp_data %>%
  mutate("residuals1nb"=residuals.glm(model1_nb, "response"))

us_cvd_sp_data %>%
  ggplot() +
  geom_sf(aes(fill=residuals1nb), color="transparent") + 
  scale_fill_gradient2(low="green", mid="white" ,high="red")

us_cvd_sp_data <- us_cvd_sp_data %>%
  mutate("residuals2nb"=residuals.glm(model2_nb, "response"))

us_cvd_sp_data %>% 
  ggplot() +
  geom_sf(aes(fill=residuals2nb), color="transparent") + 
  scale_fill_gradient2(low="green", mid="white", high="red")

moran_cor_m1nb <-sp.correlogram(neighbours=FFct_nb_w_test$neighbours,var=us_cvd_sp_data$residuals1nb,order=5,method="I",style="W", zero.policy = TRUE)
moran_cor_m2nb <-sp.correlogram(neighbours=FFct_nb_w_test$neighbours,var=us_cvd_sp_data$residuals2nb,order=5,method="I",style="W", zero.policy = TRUE)

par(mfrow=c(2,2))

plot(moran_cor_m1,ylim=c(-0.2, 0.4),main="Residuals from model 1")
plot(moran_cor_m2,ylim=c(-0.2, 0.4),main="Residuals from model 2")
plot(moran_cor_m1nb,ylim=c(-0.2, 0.4),main="Residuals from model 1, nb")
plot(moran_cor_m2nb,ylim=c(-0.2, 0.4),main="Residuals from model 2, nb")
```

```{r}
model3 <- glm(Deaths ~ median_age + 
              Perc_W_only + 
              Perc_A_only +
              Perc_Hispanic_Latino + 
              median_income +
              perc_insured + 
              `2013 Urbanization`, 
            family="poisson",
            offset = log(Population),
            data=us_cvd_sp_data,
            na.action = na.exclude)

model4 <- glm(Deaths ~ median_age + 
              median_age_lag + 
              Perc_W_only + 
              Perc_W_only_lag +
              Perc_A_only + 
              Perc_A_only_lag + 
              Perc_Hispanic_Latino + 
              Perc_Hispanic_Latino_lag + 
              median_income +
              median_income_lag +
              perc_insured +
              perc_insured_lag + 
                `2013 Urbanization`, 
            family="poisson",
            offset = log(Population),
            data=us_cvd_sp_data,
            na.action = na.exclude)
```

```{r}
us_cvd_sp_data <- us_cvd_sp_data %>%
  mutate("residuals3"=residuals.glm(model3, "response"))

us_cvd_sp_data %>%
  ggplot() +
  geom_sf(aes(fill=residuals3), color="transparent") + 
  scale_fill_gradient2(low="green", mid="white" ,high="red")

us_cvd_sp_data <- us_cvd_sp_data %>%
  mutate("residuals4"=residuals.glm(model4, "response"))

us_cvd_sp_data %>%
  ggplot() +
  geom_sf(aes(fill=residuals4), color="transparent") + 
  scale_fill_gradient2(low="green", mid="white" ,high="red")

moran_cor_m3 <-sp.correlogram(neighbours=FFct_nb_w_test$neighbours,var=us_cvd_sp_data$residuals3,order=5,method="I",style="W", zero.policy = TRUE)
moran_cor_m4 <-sp.correlogram(neighbours=FFct_nb_w_test$neighbours,var=us_cvd_sp_data$residuals4,order=5,method="I",style="W", zero.policy = TRUE)
```

Does the next set of models exhibit overdispersion?

```{r}
chi_square_model3 = sum(residuals(model3, type = "pearson")^2)

degrees_freedom_model3 = model3$df.residual

p_value_model3 = 1-pchisq(chi_square_model3, degrees_freedom_model3)

paste("Pearson goodness of fit = ", round(chi_square_model3,2), ", df = ", degrees_freedom_model3, ", p-value = ", round(p_value_model3,10))
```

```{r}
chi_square_model4 = sum(residuals(model4, type = "pearson")^2)

degrees_freedom_model4 = model4$df.residual

p_value_model4 = 1-pchisq(chi_square_model4, degrees_freedom_model4)

paste("Pearson goodness of fit = ", round(chi_square_model4,2), ", df = ", degrees_freedom_model4, ", p-value = ", round(p_value_model4,10))
```

```{r}
model3_nb <- glm.nb(Deaths ~ median_age + 
              Perc_W_only + 
              Perc_A_only + 
              Perc_Hispanic_Latino + 
              median_income +
              perc_insured + 
              `2013 Urbanization` +
                offset(log(Population)),
            data=us_cvd_sp_data,
            na.action = na.exclude)

model4_nb <- glm.nb(Deaths ~ median_age + 
              median_age_lag + 
              Perc_W_only + 
              Perc_W_only_lag + 
              Perc_A_only + 
              Perc_A_only_lag + 
              Perc_Hispanic_Latino + 
              Perc_Hispanic_Latino_lag + 
              median_income +
              median_income_lag +
              perc_insured +
              perc_insured_lag + 
              `2013 Urbanization` +
                offset(log(Population)),
            data=us_cvd_sp_data,
            na.action = na.exclude)
```

```{r}
vif(model3_nb)
vif(model4_nb)
```

```{r}
us_cvd_sp_data <- us_cvd_sp_data %>%
  mutate("residuals3nb"=residuals.glm(model3_nb, "response"))

us_cvd_sp_data %>%
  ggplot() +
  geom_sf(aes(fill=residuals3nb), color="transparent") + 
  scale_fill_gradient2(low="green", mid="white" ,high="red")

us_cvd_sp_data <- us_cvd_sp_data %>%
  mutate("residuals4nb"=residuals.glm(model4_nb, "response"))

us_cvd_sp_data %>%
  ggplot() +
  geom_sf(aes(fill=residuals4nb), color="transparent") + 
  scale_fill_gradient2(low="green", mid="white" ,high="red")

moran_cor_m3nb <-sp.correlogram(neighbours=FFct_nb_w_test$neighbours,var=us_cvd_sp_data$residuals3nb,order=5,method="I",style="W", zero.policy = TRUE)
moran_cor_m4nb <-sp.correlogram(neighbours=FFct_nb_w_test$neighbours,var=us_cvd_sp_data$residuals4nb,order=5,method="I",style="W", zero.policy = TRUE)

par(mfrow=c(2,2))

plot(moran_cor_m3,ylim=c(-0.2, 0.4),main="Residuals from model 3")
plot(moran_cor_m4,ylim=c(-0.2, 0.4),main="Residuals from model 4")
plot(moran_cor_m3nb,ylim=c(-0.2, 0.4),main="Residuals from model 3, nb")
plot(moran_cor_m4nb,ylim=c(-0.2, 0.4),main="Residuals from model 4, nb")
```

```{r}
model5 <- glm(Deaths ~ median_age + 
              median_age_lag2 + 
              Perc_W_only + 
              Perc_W_only_lag2 +
              Perc_A_only + 
              Perc_A_only_lag2 + 
              Perc_Hispanic_Latino + 
              Perc_Hispanic_Latino_lag2 + 
              median_income +
              median_income_lag2 +
              perc_insured +
              perc_insured_lag2 + 
                `2013 Urbanization`, 
            family="poisson",
            offset = log(Population),
            data=us_cvd_sp_data,
            na.action = na.exclude)

us_cvd_sp_data <- us_cvd_sp_data %>%
  mutate("residuals5"=residuals.glm(model5, "response"))

us_cvd_sp_data %>%
  ggplot() +
  geom_sf(aes(fill=residuals5), color="transparent") + 
  scale_fill_gradient2(low="green", mid="white" ,high="red")
```

```{r}
chi_square_model5 = sum(residuals(model5, type = "pearson")^2)

degrees_freedom_model5 = model5$df.residual

p_value_model5 = 1-pchisq(chi_square_model5, degrees_freedom_model5)

paste("Pearson goodness of fit = ", round(chi_square_model5,2), ", df = ", degrees_freedom_model5, ", p-value = ", round(p_value_model5,10))
```

```{r}
model5_nb <- glm.nb(Deaths ~ median_age + 
              median_age_lag2 + 
              Perc_W_only + 
              Perc_W_only_lag2 + 
              Perc_A_only + 
              Perc_A_only_lag2 + 
              Perc_Hispanic_Latino + 
              Perc_Hispanic_Latino_lag2 + 
              median_income +
              median_income_lag2 +
              perc_insured +
              perc_insured_lag2 + 
              `2013 Urbanization` +
                offset(log(Population)),
            data=us_cvd_sp_data,
            na.action = na.exclude)

us_cvd_sp_data <- us_cvd_sp_data %>%
  mutate("residuals5nb"=residuals.glm(model5_nb, "response"))

us_cvd_sp_data %>%
  ggplot() +
  geom_sf(aes(fill=residuals5nb), color="transparent") + 
  scale_fill_gradient2(low="green", mid="white" ,high="red")
```

```{r}
moran_cor_m5 <-sp.correlogram(neighbours=FFct_nb_w_test$neighbours,var=us_cvd_sp_data$residuals5,order=5,method="I",style="W", zero.policy = TRUE)
moran_cor_m5nb <-sp.correlogram(neighbours=FFct_nb_w_test$neighbours,var=us_cvd_sp_data$residuals5nb,order=5,method="I",style="W", zero.policy = TRUE)

par(mfrow=c(1,2))

plot(moran_cor_m5,ylim=c(-0.2, 0.4),main="Residuals from model 5")
plot(moran_cor_m5nb,ylim=c(-0.2, 0.4),main="Residuals from model 5, nb")
```

```{r}
model6 <- glm(Deaths ~ median_age + 
              median_age_lag2 + 
              Perc_W_only + 
              Perc_W_only_lag2 +
              Perc_A_only + 
              Perc_A_only_lag2 + 
              Perc_Hispanic_Latino + 
              Perc_Hispanic_Latino_lag2 + 
              median_income +
              median_income_lag2 +
              perc_insured +
              perc_insured_lag2 + 
                `2013 Urbanization`,
              Population_lag,
            family="poisson",
            offset = log(Population),
            data=us_cvd_sp_data,
            na.action = na.exclude)

us_cvd_sp_data <- us_cvd_sp_data %>%
  mutate("residuals6"=residuals.glm(model6, "response"))

us_cvd_sp_data %>%
  ggplot() +
  geom_sf(aes(fill=residuals6), color="transparent") + 
  scale_fill_gradient2(low="green", mid="white" ,high="red")

moran_cor_m6 <-sp.correlogram(neighbours=FFct_nb_w_test$neighbours,var=us_cvd_sp_data$residuals6,order=5,method="I",style="W", zero.policy = TRUE)
```

```{r}
chi_square_model6 = sum(residuals(model6, type = "pearson")^2)

degrees_freedom_model6 = model6$df.residual

p_value_model6 = 1-pchisq(chi_square_model6, degrees_freedom_model6)

paste("Pearson goodness of fit = ", round(chi_square_model6,2), ", df = ", degrees_freedom_model6, ", p-value = ", round(p_value_model6,10))
```

```{r}
#not the same as model6
model6_nb <- glm.nb(Deaths ~ median_age + 
              median_income + + 
              perc_insured +
                cen_dist_10km +
                nearest_LC_metro  + 
                offset(log(Population)),
            data=us_cvd_sp_data,
            na.action = na.exclude)
```
                
```{r}

us_cvd_sp_data <- us_cvd_sp_data %>%
  mutate("residuals6nb"=residuals.glm(model6_nb, "response"))

us_cvd_sp_data %>%
  ggplot() +
  geom_sf(aes(fill=residuals6nb), color="transparent") + 
  scale_fill_gradient2(low="green", mid="white" ,high="red")

moran_cor_m6nb <-sp.correlogram(neighbours=FFct_nb_w_test$neighbours,var=us_cvd_sp_data$residuals6nb,order=5,method="I",style="W", zero.policy = TRUE)

plot(moran_cor_m6,ylim=c(-0.2, 0.4),main="Residuals from model 6")
plot(moran_cor_m6nb,ylim=c(-0.2, 0.4),main="Residuals from model 6, nb")
```

```{r}
us_cvd_sp_data %>%
  ggplot() +
  geom_sf(aes(fill=cen_dist_10km), color="transparent") + 
  scale_fill_gradient2(low="white" ,high="black")
```

```{r}
vif(model6_nb)
```

```{r}
ggplot(us_cvd_sp_data, aes(x=Population, y=residuals6nb, label=County, color=`2013 Urbanization`)) +
  geom_point() + 
  geom_text()
```

```{r}
morans_i <- c(moran_cor_m1nb["res"][[1]][,1],
      moran_cor_m2nb["res"][[1]][,1],
      moran_cor_m3nb["res"][[1]][,1],
      moran_cor_m4nb["res"][[1]][,1],
      moran_cor_m5nb["res"][[1]][,1],
      moran_cor_m6nb["res"][[1]][,1]
      )

model <- c(rep("M1NB",5),
           rep("M2NB",5),
           rep("M3NB",5),
           rep("M4NB",5),
           rep("M5NB",5),
           rep("M6NB",5)
           )

lag <- rep(seq(1,5,by=1),6)

cor_df <- as.data.frame(cbind(morans_i, model, lag))

cor_df$morans_i <- as.numeric(as.character(cor_df$morans_i))

cor_df$model <- factor(cor_df$model, levels=c("M1NB",
                                                 "M2NB",
                                                 "M3NB",
                                                 "M4NB",
                                                 "M5NB",
                                              "M6NB"))

cor_df$lag <- as.numeric(as.character(cor_df$lag))

cor_df %>%
  ggplot(aes(x=lag, y=morans_i, color=model)) +
  geom_point(size=2) + 
  geom_line() + 
  geom_hline(yintercept = 0, color="black") + 
  ggtitle("Moran's I Correlogram by Model", subtitle = "Our models do not account for the spatial autocorrelation of our autocome.") + 
  facet_grid(.~model) + 
  xlim(1,5) + 
  ylim(-0.2,0.4) + 
  theme_OCS
```

We need to be careful about over-interpreting correlograms, particularly when the degree of autocorrelation is very small. 

We can test for autocorrelation globally by estimating the Moran's I statistic

```{r}
moran.mc(us_cvd_sp_data$residuals6nb, listw =FFct_nb_w_test,nsim = 999, zero.policy = TRUE)
```

Our Moran's I statistic, although significant, is close to 0. Our model appears to have accounted for residual spatial autocorrelation.

Residual spatial autocorrelation can have a large impact on the usefulness of our model. Had we not accounted for it, our standard errors would have been underestimated, amongst other things. Ultimately, such behavior in the residuals is indicative of unaccounted for information in a model.

```{r}
summary(model6_nb)
```

# Summary of results

Our model was able to account for residual spatial autocorrelation. 

# Code garage

```{r, eval=FALSE}
tm_shape(subset) +
	tm_polygons(c("Deaths", "Population", "Crude Rate", "2013 Urbanization"),
				style=c("pretty","pretty","pretty", "pretty"), breaks=list(NULL, NULL, NULL, NULL),
				palette=list("Oranges", "Purples", "Reds", "Blues"),
				border.col = "white",
				title=c("Deaths", "Population", "Crude Rate", "2013 Urbanization")) + tm_facets(as.layers = TRUE)
```

# Further Reading

[Residual spatial autocorrelation in macroecological and biogeographical modeling: a review](https://link.springer.com/article/10.1186/s41610-019-0118-3)

replace as.numeric(paste( with mean()